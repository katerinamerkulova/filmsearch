{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "name": "COMPARISON.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Cs4dUYgAAZeC",
        "_O8a8yH9l4qk",
        "wnacPTnRl4qw",
        "r5W0ULYR3hlE",
        "qCAb9MzrthA9",
        "w4goZkk-l4q2",
        "xQP7jWFgl4q3",
        "gVZxwHPLl4q3",
        "Qj9fcE0W3s4a"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs4dUYgAAZeC"
      },
      "source": [
        "### Импорт модулей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdgmyEcZ7nx-"
      },
      "source": [
        "!pip install annoy\r\n",
        "!pip3 install tensorflow_text>=2.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz5VdSxEl4qi"
      },
      "source": [
        "from annoy import AnnoyIndex # https://github.com/spotify/annoy\n",
        "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from scipy import spatial\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text\n",
        "\n",
        "\n",
        "stops = open('/content/drive/MyDrive/Colab Notebooks/russian.txt').read().split()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O8a8yH9l4qk"
      },
      "source": [
        "# Данные и оценка качества\r\n",
        "Оцениваем топ-10 поисковой выдачи"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khRwg_sYReQj"
      },
      "source": [
        "def open_data(size):\r\n",
        "    y_train, x_train, y_test, x_test = open(f'/content/drive/MyDrive/Colab Notebooks/data_format/data_{size}.txt', encoding='utf-8').read().split('\\n&&&\\n')\r\n",
        "    y_train, x_train, y_test, x_test = y_train.split('\\n'), x_train.split('\\n'), y_test.split('\\n'), x_test.split('\\n')\r\n",
        "    return y_train, x_train, y_test, x_test"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2WAGEUml4qn"
      },
      "source": [
        "def ap(relev, k):    # average presicion\n",
        "    ap = []\n",
        "    for i in range(1, k):\n",
        "        if relev[i-1] is 1:    # if doc is relevant\n",
        "            ap.append(sum(relev[:i])/i)    # summary of precisions\n",
        "    try: \n",
        "        ap = sum(ap)/sum(relev)\n",
        "    except ZeroDivisionError:\n",
        "        ap = 0.0\n",
        "    return ap\n",
        "\n",
        "def evaluation(query, relev, index=0, k=10):    # mean average precision (10)\n",
        "    prec = round(sum(relev)/k, 4)    # rank is not take into account\n",
        "    k += 1\n",
        "    avp = ap(relev, k)    # rank is take into account\n",
        "    evaluat = pd.DataFrame({'precision': prec,\n",
        "                            'average_precision': avp},\n",
        "                            index=[index])\n",
        "    return evaluat\n",
        "\n",
        "def retrieval(fit, predict, y_train, x_train, y_test, x_test):\n",
        "    df = pd.DataFrame(columns=['query', 'precision', 'average_precision'])\n",
        "    x = fit(x_train)\n",
        "    for index, query in enumerate(x_test):\n",
        "        predictions = predict(x, query, y_train)\n",
        "        relev = [0] * 10\n",
        "        for i, pred in enumerate(predictions):\n",
        "            if pred[1] == y_test[index]:\n",
        "                relev[i] = 1\n",
        "        if relev != [0] * 10:\n",
        "            df = df.append(evaluation(query, relev, index))\n",
        "        else:\n",
        "            df2 = pd.DataFrame({'precision': 0.0,\n",
        "                                'average_precision': 0.0},\n",
        "                                index=[index])\n",
        "            df = df.append(df2)\n",
        "    return df\n",
        "# map = sum(ap)/Q    # Q - number of quaries\n",
        "# recall = sum(p)/Q"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXOO6TZul4qo"
      },
      "source": [
        "# Baseline CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "syY-5TRXl4qo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dda841d-81c3-462c-935e-cd945ec8f842"
      },
      "source": [
        "vectorizers = [CountVectorizer(stop_words=stops)] * 2  # 100, 500\n",
        "vectorizers += [CountVectorizer(max_df=0.7, min_df=0.1, max_features=15000, stop_words=stops)] * 6  # 837, 1000, 5000, 10000, 20000, 30000\n",
        "\n",
        "# fitting\n",
        "def CV_fit(x_train):    \n",
        "    X = vectorizer.fit_transform(x_train)\n",
        "    return X.toarray()\n",
        "\n",
        "# similarity\n",
        "def CV_predict(x, query, y_train):\n",
        "    pred = []\n",
        "    vec = vectorizer.transform([query]).toarray()\n",
        "    simil = []\n",
        "    for vector, film in zip(x, y_train):\n",
        "        simil.append([1 - spatial.distance.cosine(vector, vec), film])\n",
        "    simil.sort(reverse=True)\n",
        "    for sim in simil[:10]:\n",
        "        pred.append(sim)\n",
        "    return pred\n",
        "\n",
        "MAP = pd.DataFrame(columns=['embedding', 'train size', 'test size', 'recall', 'MAP'])\n",
        "for i, size, vectorizer in zip((0, 1, 2, 3, 4, 5, 6, 7),\n",
        "                               (100, 500, 837, 1000, 5000, 10000, 20000, 30000),\n",
        "                               vectorizers):\n",
        "    y_train, x_train, y_test, x_test = open_data(size)\n",
        "    df = retrieval(CV_fit, CV_predict, y_train, x_train, y_test, x_test)\n",
        "    print(f'done {size}')\n",
        "    df2 = pd.DataFrame({'embedding': 'CountVectorizer',\n",
        "                        'train size': len(y_train),\n",
        "                        'test size': len(y_test),\n",
        "                        'recall': round(len([1 for prec in df.precision if prec != 0])/len(y_test), 4),\n",
        "                        'MAP': round(sum(df.average_precision)/len(y_test), 4)},\n",
        "                        index=[i])\n",
        "    MAP = MAP.append(df2)\n",
        "    MAP.to_csv('MAP.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done 100\n",
            "done 500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "done 837\n",
            "done 1000\n",
            "done 5000\n",
            "done 10000\n",
            "done 20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnacPTnRl4qw"
      },
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "bbveqjhyl4qx"
      },
      "source": [
        "vectorizers = [TfidfVectorizer(stop_words=stops)] * 2  # 100, 500\n",
        "vectorizers += [TfidfVectorizer(max_df=0.7, min_df=0.1, max_features=15000, stop_words=stops)] * 6  # 837, 1000, 5000, 10000, 20000, 30000\n",
        "\n",
        "\n",
        "# fitting\n",
        "def tfidf_fit(x_train):\n",
        "    X = vectorizer.fit_transform(x_train)\n",
        "    return X.toarray()\n",
        "\n",
        "# similarity\n",
        "def tfidf_predict(x, query, y_train):\n",
        "    pred = []\n",
        "    vec = vectorizer.transform([query]).toarray()\n",
        "    simil = []\n",
        "    for vector, film in zip(x, y_train):\n",
        "        simil.append([1 - spatial.distance.cosine(vector, vec), film])\n",
        "    simil.sort(reverse=True)\n",
        "    for sim in simil[:10]:\n",
        "        pred.append(sim)\n",
        "    return pred\n",
        "\n",
        "\n",
        "for i, size, vectorizer in zip((8, 9, 10, 11, 12, 13, 14, 15),\n",
        "                               (100, 500, 837, 1000, 5000, 10000, 20000, 30000),\n",
        "                               vectorizers):\n",
        "    y_train, x_train, y_test, x_test = open_data(size)\n",
        "    df = retrieval(tfidf_fit, tfidf_predict, y_train, x_train, y_test, x_test)\n",
        "    print(f'done {size}')\n",
        "    df2 = pd.DataFrame({'embedding': 'TF-IDF',\n",
        "                        'train size': len(y_train),\n",
        "                        'test size': len(y_test),\n",
        "                        'recall': round(len([1 for prec in df.precision if prec != 0])/len(y_test), 4),\n",
        "                        'MAP': round(sum(df.average_precision)/len(y_test), 4)},\n",
        "                        index=[i])\n",
        "    MAP = MAP.append(df2)\n",
        "    MAP.to_csv('MAP.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5W0ULYR3hlE"
      },
      "source": [
        "# Fasttext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ_Heg8a3xN5"
      },
      "source": [
        "model = gensim.models.KeyedVectors.load('/content/drive/MyDrive/Colab Notebooks/model.model') # tayga_none_fasttextcbow_300_10_2019\r\n",
        "f = 300\r\n",
        "t = AnnoyIndex(f, 'angular')  # Length of item vector that will be indexed\r\n",
        "\r\n",
        "for i, size in zip((16, 17, 18, 19, 20, 21, 22, 23),\r\n",
        "                   (100, 500, 837, 1000, 5000, 10000, 20000, 30000)):\r\n",
        "    y_train, x_train, y_test, x_test = open_data_tag(size)\r\n",
        "    for idx, text in enumerate(x_train):\r\n",
        "        vectors = []\r\n",
        "        for word in text:\r\n",
        "            try:\r\n",
        "                vectors.append(model.__getitem__(word))\r\n",
        "            except AttributeError:\r\n",
        "                continue\r\n",
        "        v = np.mean(vectors, axis=0)\r\n",
        "        try:\r\n",
        "            t.add_item(idx, v)\r\n",
        "        except ValueError:\r\n",
        "            continue\r\n",
        "\r\n",
        "    t.build(100)  # 100 trees\r\n",
        "    u = AnnoyIndex(f, 'angular')\r\n",
        "\r\n",
        "    df = pd.DataFrame(columns=['precision', 'average_precision'])\r\n",
        "\r\n",
        "    for idx, (query, film) in enumerate(zip(x_test, y_test)):\r\n",
        "        v = np.mean(vectors, axis=0)\r\n",
        "        top_10 = u.get_nns_by_vector(v, 10) # find the 10 nearest neighbors\r\n",
        "        df2 = calc_map(query, film, top_10, 10, idx)\r\n",
        "        df = df.append(df2)\r\n",
        "\r\n",
        "    df2 = pd.DataFrame({'embedding': 'Fasttext',\r\n",
        "                        'train size': len(y_train),\r\n",
        "                        'test size': len(y_test),\r\n",
        "                        'recall': round(len([1 for prec in df.precision if prec != 0])/len(y_test), 4),\r\n",
        "                        'MAP': round(sum(df.average_precision)/len(y_test), 4)},\r\n",
        "                        index=[i])\r\n",
        "    MAP = MAP.append(df2)\r\n",
        "    MAP.to_csv('MAP.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCAb9MzrthA9"
      },
      "source": [
        "# Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFXu0WQMtnbD"
      },
      "source": [
        "model = open('/content/drive/MyDrive/Colab Notebooks/model.bin', 'rb') # ruwikiruscorpora_upos_skipgram_300_2_2019\r\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(model, binary=True)\r\n",
        "\r\n",
        "def open_data_tag(size):\r\n",
        "    y_train, x_train, y_test, x_test = open(f'/content/drive/MyDrive/Colab Notebooks/data_format_tags/data_{size}.txt', encoding='utf-8').read().split('\\n&&&\\n')\r\n",
        "    y_train, x_train, y_test, x_test = y_train.split('\\n'), x_train.split('\\n'), y_test.split('\\n'), x_test.split('\\n')\r\n",
        "\r\n",
        "    for idx, text in enumerate(x_train):\r\n",
        "        x_train[idx] = [word for word in text.split() if word in model.vocab]\r\n",
        "\r\n",
        "    for idx, text in enumerate(x_test):\r\n",
        "        x_test[idx] = [word for word in text.split() if word in model.vocab]\r\n",
        "\r\n",
        "    return y_train, x_train, y_test, x_test\r\n",
        "\r\n",
        "\r\n",
        "def calc_map(query, film, predictions, k, index):\r\n",
        "    relev = [0] * k\r\n",
        "    for i, pred in enumerate(predictions):\r\n",
        "        if y_train[pred] == film:\r\n",
        "            relev[i] = 1\r\n",
        "    if relev != [0] * k:   \r\n",
        "        df = evaluation(query, relev, index, k)\r\n",
        "    else:\r\n",
        "        df = pd.DataFrame({'precision': 0.0,\r\n",
        "                           'average_precision': 0.0},\r\n",
        "                           index=[index])\r\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GSVnCIz0m8n"
      },
      "source": [
        "f = 300\r\n",
        "t = AnnoyIndex(f, 'angular')  # Length of item vector that will be indexed\r\n",
        "\r\n",
        "for i, size in zip((24, 25, 26, 27, 28, 29, 30, 31),\r\n",
        "                   (100, 500, 837, 1000, 5000, 10000, 20000, 30000)):\r\n",
        "    y_train, x_train, y_test, x_test = open_data_tag(size)\r\n",
        "    for idx, text in enumerate(x_train):\r\n",
        "        try:\r\n",
        "            v = np.mean(model[text], axis=0)\r\n",
        "        except ValueError:\r\n",
        "            continue\r\n",
        "        else:\r\n",
        "            t.add_item(idx, v)\r\n",
        "\r\n",
        "    t.build(100)  # 100 trees\r\n",
        "    u = AnnoyIndex(f, 'angular')\r\n",
        "\r\n",
        "    df = pd.DataFrame(columns=['precision', 'average_precision'])\r\n",
        "\r\n",
        "    for idx, (query, film) in enumerate(zip(x_test, y_test)):\r\n",
        "        v = np.mean(model[query], axis=0)\r\n",
        "        top_10 = u.get_nns_by_vector(v, 10) # find the 10 nearest neighbors\r\n",
        "        df2 = calc_map(query, film, top_10, 10, idx)\r\n",
        "        df = df.append(df2)\r\n",
        "\r\n",
        "    df2 = pd.DataFrame({'embedding': 'Word2Vec',\r\n",
        "                        'train size': len(y_train),\r\n",
        "                        'test size': len(y_test),\r\n",
        "                        'recall': round(len([1 for prec in df.precision if prec != 0])/len(y_test), 4),\r\n",
        "                        'MAP': round(sum(df.average_precision)/len(y_test), 4)},\r\n",
        "                        index=[i])\r\n",
        "    MAP = MAP.append(df2)\r\n",
        "    MAP.to_csv('MAP.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4goZkk-l4q2"
      },
      "source": [
        "# Doc2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQP7jWFgl4q3"
      },
      "source": [
        "##Формирование признаков и оценка качества"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "UdWlgz64l4q5"
      },
      "source": [
        "def feature_constructing(x_train, y_train):\n",
        "    tagged_data = []\n",
        "    for text, film  in zip(x_train, y_train):\n",
        "        try:\n",
        "            tagged_data.append(TaggedDocument(words=text, tags=[y_train.index(film)]))\n",
        "        except AttributeError:\n",
        "            continue\n",
        "    return tagged_data\n",
        "\n",
        "\n",
        "def doc2vec_retrieval(y_train, y_test, x_test):\n",
        "    df = pd.DataFrame(columns=['precision', 'average_precision'])\n",
        "    for index, query in enumerate(x_test):\n",
        "        test_data = [word for word in re.findall(r\"\\w+\", query.lower()) if word not in stops]\n",
        "        v1 = model.infer_vector(test_data)\n",
        "        similar_doc = model.docvecs.most_similar(positive=[v1], topn=10)\n",
        "        pred = []\n",
        "        for film, q in similar_doc:\n",
        "            pred.append([q, y_train[int(film)]])\n",
        "        relev = [0] * 10\n",
        "        for i in range(10):\n",
        "            if pred[i][1] == y_test[index]:\n",
        "                relev[i] = 1\n",
        "        if relev != [0] * 10:\n",
        "            df = df.append(evaluation(relev, index))\n",
        "        else:\n",
        "            df2 = pd.DataFrame({'precision': 0.0,\n",
        "                                'average_precision': 0.0},\n",
        "                                index=[index])\n",
        "            df = df.append(df2)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVZxwHPLl4q3"
      },
      "source": [
        "## Обучение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg5UQBGqC9L3"
      },
      "source": [
        "max_epochs = 10\r\n",
        "vec_size = 50\r\n",
        "\r\n",
        "for i, size in zip((32, 33, 34, 35, 36, 37, 38, 39),\r\n",
        "                   (100, 500, 837, 1000, 5000, 10000, 20000, 30000)):\r\n",
        "    model = Doc2Vec(vector_size=vec_size,\r\n",
        "                    min_count=0.1,\r\n",
        "                    epochs=max_epochs)\r\n",
        "    y_train, x_train, y_test, x_test = open_data(size)\r\n",
        "    tagged_data = feature_constructing(x_train, y_train)\r\n",
        "    model.build_vocab(tagged_data)\r\n",
        "    model.train(tagged_data, \r\n",
        "                total_examples=model.corpus_count, \r\n",
        "                epochs=model.epochs)\r\n",
        "\r\n",
        "    df = doc2vec_retrieval(y_train, y_test, x_test)\r\n",
        "    df2 = pd.DataFrame({'embedding': 'Doc2Vec',\r\n",
        "                        'train size': len(y_train),\r\n",
        "                        'test size': len(y_test),\r\n",
        "                        'recall': round(len([1 for prec in df.precision if prec != 0])/len(y_test), 4),\r\n",
        "                        'MAP': round(sum(df.average_precision)/len(y_test), 4)},\r\n",
        "                        index=[i])\r\n",
        "    MAP = MAP.append(df2)\r\n",
        "    MAP.to_csv('MAP.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj9fcE0W3s4a"
      },
      "source": [
        "# Universal Sentence Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zabi5rKn3yNh"
      },
      "source": [
        "model = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3')\r\n",
        "f = 512\r\n",
        "t = AnnoyIndex(f, 'angular')  # Length of item vector that will be indexed\r\n",
        "\r\n",
        "for i, size in zip((40, 41, 42, 43, 44, 45, 46, 47),\r\n",
        "                   (100, 500, 837, 1000, 5000, 10000, 20000, 30000)):\r\n",
        "    y_train, x_train, y_test, x_test = open_data_tag(size)\r\n",
        "    for idx, text in enumerate(x_train):\r\n",
        "        v = model(text)[0]\r\n",
        "        try:\r\n",
        "            t.add_item(idx, v)\r\n",
        "        except ValueError:\r\n",
        "            continue\r\n",
        "\r\n",
        "    t.build(100)  # 100 trees\r\n",
        "    u = AnnoyIndex(f, 'angular')\r\n",
        "\r\n",
        "    df = pd.DataFrame(columns=['precision', 'average_precision'])\r\n",
        "\r\n",
        "    for idx, (query, film) in enumerate(zip(x_test, y_test)):\r\n",
        "        v = model(query)[0]\r\n",
        "        top_10 = u.get_nns_by_vector(v, 10) # find the 10 nearest neighbors\r\n",
        "        df2 = calc_map(query, film, top_10, 10, idx)\r\n",
        "        df = df.append(df2)\r\n",
        "\r\n",
        "    df2 = pd.DataFrame({'embedding': 'Fasttext',\r\n",
        "                        'train size': len(y_train),\r\n",
        "                        'test size': len(y_test),\r\n",
        "                        'recall': round(len([1 for prec in df.precision if prec != 0])/len(y_test), 4),\r\n",
        "                        'MAP': round(sum(df.average_precision)/len(y_test), 4)},\r\n",
        "                        index=[i])\r\n",
        "    MAP = MAP.append(df2)\r\n",
        "    MAP.to_csv('MAP.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lus2JFxAEsX4"
      },
      "source": [
        "MAP"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}