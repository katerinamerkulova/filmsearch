{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMPARISON.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Cs4dUYgAAZeC",
        "_O8a8yH9l4qk",
        "qCAb9MzrthA9",
        "w4goZkk-l4q2"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs4dUYgAAZeC"
      },
      "source": [
        "### import stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdgmyEcZ7nx-",
        "outputId": "6f2eace3-f8a2-47e9-f83c-fc33dfa14d63"
      },
      "source": [
        "!pip install annoy  # Locality Sensitive Hashing https://github.com/spotify/annoy\n",
        "!pip3 install tensorflow_text>=2.0.0"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting annoy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/5b/1c22129f608b3f438713b91cd880dc681d747a860afe3e8e0af86e921942/annoy-1.17.0.tar.gz (646kB)\n",
            "\r\u001b[K     |▌                               | 10kB 19.3MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 23.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 12.5MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 10.2MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51kB 7.9MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 8.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 71kB 8.0MB/s eta 0:00:01\r\u001b[K     |████                            | 81kB 8.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 102kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 122kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 133kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 143kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 153kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 163kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 174kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 184kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 194kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 204kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 215kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 225kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 235kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 245kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 256kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 266kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 276kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 286kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 296kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 307kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 317kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 327kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 337kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 348kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 358kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 368kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 378kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 389kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 399kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 409kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 419kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 430kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 440kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 450kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 460kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 471kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 481kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 491kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 501kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 512kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 522kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 532kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 542kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 552kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 563kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 573kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 583kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 593kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 604kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 614kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 624kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 634kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 655kB 9.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp37-cp37m-linux_x86_64.whl size=394497 sha256=05afc42598021c571eec60776856f370d8d3f19aa0bbce6ad05d7d79a5773b93\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/c5/59/cce7e67b52c8e987389e53f917b6bb2a9d904a03246fadcb1e\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz5VdSxEl4qi"
      },
      "source": [
        "from annoy import AnnoyIndex\n",
        "import gensim\n",
        "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import spatial\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text\n",
        "from time import process_time\n",
        "\n",
        "stops = open('/content/drive/MyDrive/Colab Notebooks/russian.txt').read().split()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O8a8yH9l4qk"
      },
      "source": [
        "# Data & Metrics\n",
        "estimate by top-10 search result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khRwg_sYReQj"
      },
      "source": [
        "def open_data(size):\n",
        "    y_train, x_train, y_test, x_test = open(f'/content/drive/MyDrive/Colab Notebooks/data_format/data_{size}.txt', encoding='utf-8').read().split('\\n&&&\\n')\n",
        "    y_train, x_train, y_test, x_test = y_train.split('\\n'), x_train.split('\\n'), y_test.split('\\n'), x_test.split('\\n')\n",
        "    return y_train, x_train, y_test, x_test"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2WAGEUml4qn"
      },
      "source": [
        "def ap(relev, k):    # average precision\n",
        "    ap = []\n",
        "    for i in range(1, k + 1):\n",
        "        if relev[i-1] is 1:    # if doc is relevant\n",
        "            ap.append(sum(relev[:i])/i)    # summary of precisions\n",
        "    try: \n",
        "        ap = sum(ap)/sum(relev)\n",
        "    except ZeroDivisionError:\n",
        "        ap = 0.0\n",
        "    return ap\n",
        "\n",
        "def evaluation(relev, index=0, k=10):    # mean average precision (10)\n",
        "    prec = round(sum(relev)/k, 4)    # rank is not take into account\n",
        "    avp = ap(relev, k)    # rank is take into account\n",
        "    evaluat = pd.DataFrame({'recall': prec,\n",
        "                            'average_precision': avp},\n",
        "                            index=[index])\n",
        "    return evaluat\n",
        "\n",
        "def retrieval(fit, predict, y_train, x_train, y_test, x_test):\n",
        "    y_test = [film.split(';') for film in y_test]\n",
        "    df = pd.DataFrame(columns=['recall', 'average_precision'])\n",
        "    train_start = process_time()\n",
        "    x = fit(x_train)\n",
        "    train_stop = process_time()\n",
        "    train_time = train_stop - train_start\n",
        "\n",
        "    test_start = process_time()\n",
        "    for index, query in enumerate(x_test):\n",
        "        predictions = predict(x, query, y_train)\n",
        "        relev = [0] * 10\n",
        "        for i, pred in enumerate(predictions):\n",
        "            if pred[1] in y_test[index]:\n",
        "                relev[i] = 1\n",
        "        if relev != [0] * 10:\n",
        "            df = df.append(evaluation(relev, index))\n",
        "        else:\n",
        "            df2 = pd.DataFrame({'recall': 0.0,\n",
        "                                'average_precision': 0.0},\n",
        "                                index=[index])\n",
        "            df = df.append(df2)\n",
        "    test_stop = process_time()\n",
        "    test_time = test_stop - test_start\n",
        "    return df, train_time, test_time\n",
        "# map = sum(ap)/Q    # Q - number of quaries\n",
        "# recall = sum(p)/Q\n",
        "MAP = pd.DataFrame(columns=[\n",
        "  'embedding',\n",
        "  'train size',\n",
        "  'test size', \n",
        "  'training time', \n",
        "  'inference time', \n",
        "  'recall', \n",
        "  'MAP'\n",
        "])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXOO6TZul4qo"
      },
      "source": [
        "# Baseline: Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syY-5TRXl4qo",
        "scrolled": true,
        "outputId": "b88759da-4c30-4207-da5c-5f1578600b2d"
      },
      "source": [
        "vectorizers = [CountVectorizer(stop_words=stops)] * 4  # 100, 500, 1000, 5000\n",
        "vectorizers += [CountVectorizer(max_features=30000, stop_words=stops)] * 3  # 10000, 20000, 30000\n",
        "\n",
        "# fitting\n",
        "def CV_fit(x_train):    \n",
        "    X = vectorizer.fit_transform(x_train)\n",
        "    return X.toarray()\n",
        "\n",
        "# similarity\n",
        "def CV_predict(x, query, y_train):\n",
        "    pred = []\n",
        "    vec = vectorizer.transform([query]).toarray()\n",
        "    simil = []\n",
        "    for vector, film in zip(x, y_train):\n",
        "        simil.append([1 - spatial.distance.cosine(vector, vec), film])\n",
        "    simil.sort(reverse=True)\n",
        "    for sim in simil[:10]:\n",
        "        pred.append(sim)\n",
        "    return pred\n",
        "\n",
        "for i, size, vectorizer in zip((0, 1, 2, 3, 4, 5, 6),\n",
        "                               (100, 500, 1000, 5000, 10000, 20000, 30000),\n",
        "                               vectorizers):\n",
        "    y_train, x_train, y_test, x_test = open_data(size)\n",
        "    df, train_time, test_time = retrieval(CV_fit, CV_predict, y_train, x_train, y_test, x_test)\n",
        "    print(f'done {size}')\n",
        "    df2 = pd.DataFrame({'embedding': 'CountVectorizer',\n",
        "                        'train size': len(y_train),\n",
        "                        'test size': len(y_test),\n",
        "                        'training time': round(train_time, 2),\n",
        "                        'inference time': round(test_time / len(y_test), 2),\n",
        "                        'recall': round(len([1 for prec in df.recall if prec != 0])/len(y_test), 4),\n",
        "                        'MAP': round(sum(df.average_precision)/len(y_test), 4)},\n",
        "                        index=[i])\n",
        "    MAP = MAP.append(df2)\n",
        "    MAP.to_csv('/content/drive/MyDrive/Colab Notebooks/MAP_new.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done 100\n",
            "done 500\n",
            "done 1000\n",
            "done 5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "done 10000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "done 20000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "done 30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnacPTnRl4qw"
      },
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbveqjhyl4qx",
        "scrolled": true
      },
      "source": [
        "vectorizers = [TfidfVectorizer(stop_words=stops)] * 4 # 100, 500, 1000, 5000\n",
        "vectorizers += [TfidfVectorizer(max_features=30000, stop_words=stops)] * 3  # 10000, 20000, 30000\n",
        " \n",
        "# fitting\n",
        "def tfidf_fit(x_train):\n",
        "    X = vectorizer.fit_transform(x_train)\n",
        "    return X.toarray()\n",
        " \n",
        "# similarity\n",
        "def tfidf_predict(x, query, y_train):\n",
        "    pred = []\n",
        "    vec = vectorizer.transform([query]).toarray()\n",
        "    simil = []\n",
        "    for vector, film in zip(x, y_train):\n",
        "        simil.append([1 - spatial.distance.cosine(vector, vec), film])\n",
        "    simil.sort(reverse=True)\n",
        "    for sim in simil[:10]:\n",
        "        pred.append(sim)\n",
        "    return pred\n",
        " \n",
        "for i, size, vectorizer in zip((7, 8, 9, 10, 11, 12, 13),\n",
        "                               (100, 500, 1000, 5000, 10000, 20000, 30000),\n",
        "                               vectorizers):\n",
        "    y_train, x_train, y_test, x_test = open_data(size)\n",
        "    df, train_time, test_time = retrieval(tfidf_fit, tfidf_predict, y_train, x_train, y_test, x_test)\n",
        "    print(f'done {size}')\n",
        "    df2 = pd.DataFrame({'embedding': 'TF-IDF',\n",
        "                        'train size': len(y_train),\n",
        "                        'test size': len(y_test),\n",
        "                        'training time': round(train_time, 2),\n",
        "                        'inference time': round(test_time / len(y_test), 2),\n",
        "                        'recall': round(len([1 for prec in df.recall if prec != 0])/len(y_test), 4),\n",
        "                        'MAP': round(sum(df.average_precision)/len(y_test), 4)},\n",
        "                        index=[i])\n",
        "    MAP = MAP.append(df2)\n",
        "    MAP.to_csv('/content/drive/MyDrive/Colab Notebooks/MAP_new.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5W0ULYR3hlE"
      },
      "source": [
        "# Fasttext (pre-trained) & Locality Sensitive Hashing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIqvQ2DNb-GA"
      },
      "source": [
        "def calc_map(films, predictions, k, index):\n",
        "    films = films.split(';')\n",
        "    relev = [0] * k\n",
        "    for i, pred in enumerate(predictions):\n",
        "        if y_train[pred] in films:\n",
        "            relev[i] = 1\n",
        "    if relev != [0] * k:   \n",
        "        df = evaluation(relev, index, k)\n",
        "    else:\n",
        "        df = pd.DataFrame({'precision': 0.0,\n",
        "                           'average_precision': 0.0},\n",
        "                           index=[index])\n",
        "    return df"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ_Heg8a3xN5",
        "outputId": "b6ebb2a4-b723-4724-bf4a-fa4ba39f0c28"
      },
      "source": [
        "import re\n",
        "\n",
        "model = gensim.models.KeyedVectors.load('/content/drive/MyDrive/Colab Notebooks/model.model') # tayga_none_fasttextcbow_300_10_2019\n",
        "\n",
        "for i, size in zip((14, 15, 16, 17, 18, 19, 20),\n",
        "                   (100, 500, 1000, 5000, 10000, 20000, 30000)):\n",
        "    f = 300\n",
        "    t = AnnoyIndex(f, 'angular')  # Length of item vector that will be indexed\n",
        "    y_train, x_train, y_test, x_test = open_data(size)\n",
        "    x_train = [re.findall('\\w+', text) for text in x_train]\n",
        "    x_test = [re.findall('\\w+', text) for text in x_test]\n",
        "\n",
        "    train_start = process_time()\n",
        "    for idx, text in enumerate(x_train):\n",
        "        vectors = []\n",
        "        for word in text:\n",
        "            try:\n",
        "                vectors.append(model.__getitem__(word))\n",
        "            except AttributeError:\n",
        "                continue\n",
        "        v = np.mean(vectors, axis=0)\n",
        "        try:\n",
        "            t.add_item(idx, v)\n",
        "        except ValueError:\n",
        "            continue\n",
        "    train_stop = process_time()\n",
        "    train_time = train_stop - train_start\n",
        "    t.build(100)  # 100 trees\n",
        "\n",
        "    df = pd.DataFrame(columns=['precision', 'average_precision'])\n",
        "    test_start = process_time()\n",
        "    for idx, (query, films) in enumerate(zip(x_test, y_test)):\n",
        "        vectors = []\n",
        "        for word in query:\n",
        "            try:\n",
        "                vectors.append(model.__getitem__(word))\n",
        "            except AttributeError:\n",
        "                continue\n",
        "        v = np.mean(vectors, axis=0)\n",
        "        top_10 = t.get_nns_by_vector(v, 10)  # find the 10 nearest neighbors\n",
        "        df2 = calc_map(films, top_10, 10, idx)\n",
        "        df = df.append(df2)\n",
        "    test_stop = process_time()\n",
        "    test_time = test_stop - test_start\n",
        "\n",
        "    print(f'done {size}')\n",
        "    df2 = pd.DataFrame({'embedding': 'Fasttext',\n",
        "                        'train size': len(y_train),\n",
        "                        'test size': len(y_test),\n",
        "                        'training time': round(train_time, 2),\n",
        "                        'inference time': round(test_time / len(y_test), 2),\n",
        "                        'recall': round(len([1 for prec in df.precision if prec != 0])/len(y_test), 4),\n",
        "                        'MAP': round(sum(df.average_precision)/len(y_test), 4)},\n",
        "                        index=[i])\n",
        "    MAP = MAP.append(df2)\n",
        "    MAP.to_csv('/content/drive/MyDrive/Colab Notebooks/MAP_new.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done 100\n",
            "done 500\n",
            "done 1000\n",
            "done 5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "done 10000\n",
            "done 20000\n",
            "done 30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCAb9MzrthA9"
      },
      "source": [
        "# Word2Vec (pre-trained) & Locality Sensitive Hashing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJH_fhPYfYKa"
      },
      "source": [
        "def open_data_tag(size):\n",
        "    y_train, x_train, y_test, x_test = open(f'/content/drive/MyDrive/Colab Notebooks/data_format_tags/data_{size}.txt', encoding='utf-8').read().split('\\n&&&\\n')\n",
        "    y_train, x_train, y_test, x_test = y_train.split('\\n'), x_train.split('\\n'), y_test.split('\\n'), x_test.split('\\n')\n",
        "\n",
        "    for idx, text in enumerate(x_train):\n",
        "        x_train[idx] = [word for word in text.split() if word in model.vocab]\n",
        "\n",
        "    for idx, text in enumerate(x_test):\n",
        "        x_test[idx] = [word for word in text.split() if word in model.vocab]\n",
        "\n",
        "    return y_train, x_train, y_test, x_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GSVnCIz0m8n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "252ec157-39f3-4887-a3f2-ae915197aa7e"
      },
      "source": [
        "model = open('/content/drive/MyDrive/Colab Notebooks/model.bin', 'rb') # ruwikiruscorpora_upos_skipgram_300_2_2019\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(model, binary=True)\n",
        "\n",
        "for i, size in zip((21, 22, 23, 24, 25, 26, 27),\n",
        "                   (100, 500, 1000, 5000, 10000, 20000, 30000)):\n",
        "    f = 300\n",
        "    t = AnnoyIndex(f, 'angular')  # Length of item vector that will be indexed\n",
        "    y_train, x_train, y_test, x_test = open_data_tag(size)\n",
        "    train_start = process_time()\n",
        "    for idx, text in enumerate(x_train):\n",
        "        try:\n",
        "            v = np.mean(model[text], axis=0)\n",
        "        except ValueError:\n",
        "            continue\n",
        "        else:\n",
        "            t.add_item(idx, v)\n",
        "    train_stop = process_time()\n",
        "    train_time = train_stop - train_start\n",
        "    t.build(100)  # 100 trees\n",
        "\n",
        "    df = pd.DataFrame(columns=['precision', 'average_precision'])\n",
        "    test_start = process_time()\n",
        "    for idx, (query, films) in enumerate(zip(x_test, y_test)):\n",
        "        v = np.mean(model[query], axis=0)\n",
        "        top_10 = t.get_nns_by_vector(v, 10) # find the 10 nearest neighbors\n",
        "        df2 = calc_map(films, top_10, 10, idx)\n",
        "        df = df.append(df2)\n",
        "    test_stop = process_time()\n",
        "    test_time = test_stop - test_start\n",
        "\n",
        "    print(f'done {size}')\n",
        "    df2 = pd.DataFrame({'embedding': 'Word2Vec',\n",
        "                        'train size': len(y_train),\n",
        "                        'test size': len(y_test),\n",
        "                        'training time': round(train_time, 2),\n",
        "                        'inference time': round(test_time / len(y_test), 2),\n",
        "                        'recall': round(len([1 for prec in df.precision if prec != 0])/len(y_test), 4),\n",
        "                        'MAP': round(sum(df.average_precision)/len(y_test), 4)},\n",
        "                        index=[i])\n",
        "    MAP = MAP.append(df2)\n",
        "    MAP.to_csv('/content/drive/MyDrive/Colab Notebooks/MAP_new.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done 100\n",
            "done 500\n",
            "done 1000\n",
            "done 5000\n",
            "done 10000\n",
            "done 20000\n",
            "done 30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4goZkk-l4q2"
      },
      "source": [
        "# Doc2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdWlgz64l4q5",
        "scrolled": true
      },
      "source": [
        "def feature_constructing(x_train, y_train):\n",
        "    tagged_data = []\n",
        "    for text, film  in zip(x_train, y_train):\n",
        "        try:\n",
        "            tagged_data.append(TaggedDocument(words=text, tags=[y_train.index(film)]))\n",
        "        except AttributeError:\n",
        "            continue\n",
        "    return tagged_data\n",
        "\n",
        "\n",
        "def doc2vec_retrieval(y_train, y_test, x_test):\n",
        "    y_test = [films.split(';') for films in y_test]\n",
        "    df = pd.DataFrame(columns=['precision', 'average_precision'])\n",
        "    for index, query in enumerate(x_test):\n",
        "        test_data = [word for word in re.findall(r\"\\w+\", query.lower()) if word not in stops]\n",
        "        v1 = model.infer_vector(test_data)\n",
        "        similar_doc = model.docvecs.most_similar(positive=[v1], topn=10)\n",
        "        pred = []\n",
        "        for film, q in similar_doc:\n",
        "            pred.append([q, y_train[int(film)]])\n",
        "        relev = [0] * 10\n",
        "        for i in range(10):\n",
        "            if pred[i][1] in y_test[index]:\n",
        "                relev[i] = 1\n",
        "        if relev != [0] * 10:\n",
        "            df = df.append(evaluation(relev, index))\n",
        "        else:\n",
        "            df2 = pd.DataFrame({'precision': 0.0,\n",
        "                                'average_precision': 0.0},\n",
        "                                index=[index])\n",
        "            df = df.append(df2)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg5UQBGqC9L3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d379d096-eb3b-46ac-f081-5225f7da6d36"
      },
      "source": [
        "max_epochs = 10\n",
        "vec_size = 50\n",
        "\n",
        "for i, size in zip((28, 29, 30, 31, 32, 33, 34),\n",
        "                   (100, 500, 1000, 5000, 10000, 20000, 30000)):\n",
        "    model = Doc2Vec(vector_size=vec_size,\n",
        "                    min_count=0.1,\n",
        "                    epochs=max_epochs)\n",
        "    y_train, x_train, y_test, x_test = open_data(size)\n",
        "    tagged_data = feature_constructing(x_train, y_train)\n",
        "    model.build_vocab(tagged_data)\n",
        "    train_start = process_time()\n",
        "    model.train(tagged_data, \n",
        "                total_examples=model.corpus_count, \n",
        "                epochs=model.epochs)\n",
        "    train_stop = process_time()\n",
        "    train_time = train_stop - train_start\n",
        "\n",
        "    test_start = process_time()\n",
        "    df = doc2vec_retrieval(y_train, y_test, x_test)\n",
        "    test_stop = process_time()\n",
        "    test_time = test_stop - test_start\n",
        "    print(f'done {size}')\n",
        "    df2 = pd.DataFrame({'embedding': 'Doc2Vec',\n",
        "                        'train size': len(y_train),\n",
        "                        'test size': len(y_test),\n",
        "                        'training time': round(train_time, 2),\n",
        "                        'inference time': round(test_time / len(y_test), 2),\n",
        "                        'recall': round(len([1 for prec in df.precision if prec != 0])/len(y_test), 4),\n",
        "                        'MAP': round(sum(df.average_precision)/len(y_test), 4)},\n",
        "                        index=[i])\n",
        "    MAP = MAP.append(df2)\n",
        "    MAP.to_csv('/content/drive/MyDrive/Colab Notebooks/MAP_new.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done 100\n",
            "done 500\n",
            "done 1000\n",
            "done 5000\n",
            "done 10000\n",
            "done 20000\n",
            "done 30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj9fcE0W3s4a"
      },
      "source": [
        "# Universal Sentence Encoder (pre-trained) & Locality Sensitive Hashing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zabi5rKn3yNh"
      },
      "source": [
        "model = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3')\n",
        " \n",
        "for i, size in zip((35, 36, 37, 38, 39, 40, 41), \n",
        "                   (100, 500, 1000, 5000, 10000, 20000, 30000)):\n",
        "    f = 512\n",
        "    t = AnnoyIndex(f, 'angular')  # Length of item vector that will be indexed\n",
        "    y_train, x_train, y_test, x_test = open_data(size)\n",
        "    train_start = process_time()\n",
        "    for idx, text in enumerate(x_train):\n",
        "        v = model(text)[0]\n",
        "        try:\n",
        "            t.add_item(idx, v)\n",
        "        except ValueError:\n",
        "            continue\n",
        "    train_stop = process_time()\n",
        "    train_time = train_stop - train_start\n",
        " \n",
        "    t.build(100)  # 100 trees\n",
        "    df = pd.DataFrame(columns=['precision', 'average_precision'])\n",
        " \n",
        "    test_start = process_time()\n",
        "    for idx, (query, films) in enumerate(zip(x_test, y_test)):\n",
        "        v = model(query)[0]\n",
        "        top_10 = t.get_nns_by_vector(v, 10) # find the 10 nearest neighbors\n",
        "        df2 = calc_map(films, top_10, 10, idx)\n",
        "        df = df.append(df2)\n",
        "    test_stop = process_time()\n",
        "    test_time = test_stop - test_start\n",
        " \n",
        "    print(f'done {size}')\n",
        "    df2 = pd.DataFrame({'embedding': 'Universal Sentence Encoder',\n",
        "                        'train size': len(y_train),\n",
        "                        'test size': len(y_test),\n",
        "                        'training time': round(train_time, 2),\n",
        "                        'inference time': round(test_time / len(y_test), 2),\n",
        "                        'recall': round(len([1 for prec in df.precision if prec != 0])/len(y_test), 4),\n",
        "                        'MAP': round(sum(df.average_precision)/len(y_test), 4)},\n",
        "                        index=[i])\n",
        "    MAP = MAP.append(df2)\n",
        "    MAP.to_csv('/content/drive/MyDrive/Colab Notebooks/MAP_new.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}