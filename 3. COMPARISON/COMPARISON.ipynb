{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMPARISON.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Cs4dUYgAAZeC",
        "_O8a8yH9l4qk",
        "wnacPTnRl4qw",
        "r5W0ULYR3hlE",
        "qCAb9MzrthA9",
        "w4goZkk-l4q2"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs4dUYgAAZeC"
      },
      "source": [
        "### import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdgmyEcZ7nx-",
        "outputId": "c312a915-1c2d-4a29-ff95-33c00ec198f7"
      },
      "source": [
        "!pip install annoy  # Locality Sensitive Hashing https://github.com/spotify/annoy\r\n",
        "!pip3 install tensorflow_text>=2.0.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting annoy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/5b/1c22129f608b3f438713b91cd880dc681d747a860afe3e8e0af86e921942/annoy-1.17.0.tar.gz (646kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 5.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp36-cp36m-linux_x86_64.whl size=390360 sha256=256b6d8d74db10e434ed62e70735a02c4b9b6b407a9c84fd9028abe526e8257e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/c5/59/cce7e67b52c8e987389e53f917b6bb2a9d904a03246fadcb1e\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz5VdSxEl4qi"
      },
      "source": [
        "from annoy import AnnoyIndex\n",
        "import gensim\n",
        "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from scipy import spatial\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text\n",
        "from time import process_time\n",
        "\n",
        "stops = open('/content/drive/MyDrive/Colab Notebooks/russian.txt').read().split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O8a8yH9l4qk"
      },
      "source": [
        "# Data & Metrics\r\n",
        "estimate by top-10 search result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khRwg_sYReQj"
      },
      "source": [
        "def open_data(size):\r\n",
        "    y_train, x_train, y_test, x_test = open(f'/content/drive/MyDrive/Colab Notebooks/data_format/data_{size}.txt', encoding='utf-8').read().split('\\n&&&\\n')\r\n",
        "    y_train, x_train, y_test, x_test = y_train.split('\\n'), x_train.split('\\n'), y_test.split('\\n'), x_test.split('\\n')\r\n",
        "    return y_train, x_train, y_test, x_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2WAGEUml4qn"
      },
      "source": [
        "def ap(relev, k):    # average presicion\n",
        "    ap = []\n",
        "    for i in range(1, k):\n",
        "        if relev[i-1] is 1:    # if doc is relevant\n",
        "            ap.append(sum(relev[:i])/i)    # summary of precisions\n",
        "    try: \n",
        "        ap = sum(ap)/sum(relev)\n",
        "    except ZeroDivisionError:\n",
        "        ap = 0.0\n",
        "    return ap\n",
        "\n",
        "def evaluation(query, relev, index=0, k=10):    # mean average precision (10)\n",
        "    prec = round(sum(relev)/k, 4)    # rank is not take into account\n",
        "    k += 1\n",
        "    avp = ap(relev, k)    # rank is take into account\n",
        "    evaluat = pd.DataFrame({'precision': prec,\n",
        "                            'average_precision': avp},\n",
        "                            index=[index])\n",
        "    return evaluat\n",
        "\n",
        "def retrieval(fit, predict, y_train, x_train, y_test, x_test):\n",
        "    df = pd.DataFrame(columns=['query', 'precision', 'average_precision'])\n",
        "    train_start = process_time()\n",
        "    x = fit(x_train)\n",
        "    train_stop = process_time()\n",
        "    train_time = train_stop - train_start\n",
        "\n",
        "    test_start = process_time()\n",
        "    for index, query in enumerate(x_test):\n",
        "        predictions = predict(x, query, y_train)\n",
        "        relev = [0] * 10\n",
        "        for i, pred in enumerate(predictions):\n",
        "            if pred[1] == y_test[index]:\n",
        "                relev[i] = 1\n",
        "        if relev != [0] * 10:\n",
        "            df = df.append(evaluation(query, relev, index))\n",
        "        else:\n",
        "            df2 = pd.DataFrame({'precision': 0.0,\n",
        "                                'average_precision': 0.0},\n",
        "                                index=[index])\n",
        "            df = df.append(df2)\n",
        "    test_stop = process_time()\n",
        "    test_time = test_stop - test_start\n",
        "    return df, train_time, test_time\n",
        "# map = sum(ap)/Q    # Q - number of quaries\n",
        "# recall = sum(p)/Q\n",
        "MAP = pd.DataFrame(columns=[\n",
        "  'embedding',\n",
        "  'train size',\n",
        "  'test size', \n",
        "  'training time', \n",
        "  'inference time', \n",
        "  'recall', \n",
        "  'MAP'\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXOO6TZul4qo"
      },
      "source": [
        "# Baseline: Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "syY-5TRXl4qo",
        "scrolled": true,
        "outputId": "d66e8b1e-3b3d-451a-84bd-2fbf779147f6"
      },
      "source": [
        "vectorizers = [CountVectorizer(stop_words=stops)] * 2  # 100, 500\n",
        "vectorizers += [CountVectorizer(max_df=0.7, min_df=0.1, max_features=15000, stop_words=stops)] * 6  # 837, 1000, 5000, 10000, 20000, 30000\n",
        "\n",
        "# fitting\n",
        "def CV_fit(x_train):    \n",
        "    X = vectorizer.fit_transform(x_train)\n",
        "    return X.toarray()\n",
        "\n",
        "# similarity\n",
        "def CV_predict(x, query, y_train):\n",
        "    pred = []\n",
        "    vec = vectorizer.transform([query]).toarray()\n",
        "    simil = []\n",
        "    for vector, film in zip(x, y_train):\n",
        "        simil.append([1 - spatial.distance.cosine(vector, vec), film])\n",
        "    simil.sort(reverse=True)\n",
        "    for sim in simil[:10]:\n",
        "        pred.append(sim)\n",
        "    return pred\n",
        "\n",
        "for i, size, vectorizer in zip((0, 1, 2, 3, 4, 5, 6, 7),\n",
        "                               (100, 500, 837, 1000, 5000, 10000, 20000, 30000),\n",
        "                               vectorizers):\n",
        "    y_train, x_train, y_test, x_test = open_data(size)\n",
        "    df, train_time, test_time = retrieval(CV_fit, CV_predict, y_train, x_train, y_test, x_test)\n",
        "    print(f'done {size}')\n",
        "    df2 = pd.DataFrame({'embedding': 'CountVectorizer',\n",
        "                        'train size': len(y_train),\n",
        "                        'test size': len(y_test),\n",
        "                        'training time': round(train_time, 2),\n",
        "                        'inference time': round(test_time / len(y_test), 2),\n",
        "                        'recall': round(len([1 for prec in df.precision if prec != 0])/len(y_test), 4),\n",
        "                        'MAP': round(sum(df.average_precision)/len(y_test), 4)},\n",
        "                        index=[i])\n",
        "    MAP = MAP.append(df2)\n",
        "    MAP.to_csv('/content/drive/MyDrive/Colab Notebooks/MAP.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done 100\n",
            "done 500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "done 837\n",
            "done 1000\n",
            "done 5000\n",
            "done 10000\n",
            "done 20000\n",
            "done 30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnacPTnRl4qw"
      },
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbveqjhyl4qx",
        "scrolled": true
      },
      "source": [
        "vectorizers = [TfidfVectorizer(stop_words=stops)] * 2  # 100, 500\n",
        "vectorizers += [TfidfVectorizer(max_df=0.7, min_df=0.1, max_features=15000, stop_words=stops)] * 6  # 837, 1000, 5000, 10000, 20000, 30000\n",
        "\n",
        "# fitting\n",
        "def tfidf_fit(x_train):\n",
        "    X = vectorizer.fit_transform(x_train)\n",
        "    return X.toarray()\n",
        "\n",
        "# similarity\n",
        "def tfidf_predict(x, query, y_train):\n",
        "    pred = []\n",
        "    vec = vectorizer.transform([query]).toarray()\n",
        "    simil = []\n",
        "    for vector, film in zip(x, y_train):\n",
        "        simil.append([1 - spatial.distance.cosine(vector, vec), film])\n",
        "    simil.sort(reverse=True)\n",
        "    for sim in simil[:10]:\n",
        "        pred.append(sim)\n",
        "    return pred\n",
        "\n",
        "\n",
        "for i, size, vectorizer in zip((8, 9, 10, 11, 12, 13, 14, 15),\n",
        "                               (100, 500, 837, 1000, 5000, 10000, 20000, 30000),\n",
        "                               vectorizers):\n",
        "    y_train, x_train, y_test, x_test = open_data(size)\n",
        "    df, train_time, test_time  = retrieval(tfidf_fit, tfidf_predict, y_train, x_train, y_test, x_test)\n",
        "    print(f'done {size}')\n",
        "    df2 = pd.DataFrame({'embedding': 'TF-IDF',\n",
        "                        'train size': len(y_train),\n",
        "                        'test size': len(y_test),\n",
        "                        'training time': round(train_time, 2),\n",
        "                        'inference time': round(test_time / len(y_test), 2),\n",
        "                        'recall': round(len([1 for prec in df.precision if prec != 0])/len(y_test), 4),\n",
        "                        'MAP': round(sum(df.average_precision)/len(y_test), 4)},\n",
        "                        index=[i])\n",
        "    MAP = MAP.append(df2)\n",
        "    MAP.to_csv('/content/drive/MyDrive/Colab Notebooks/MAP.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5W0ULYR3hlE"
      },
      "source": [
        "# Fasttext (pre-trained) & Locality Sensitive Hashing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIqvQ2DNb-GA"
      },
      "source": [
        "def calc_map(query, film, predictions, k, index):\r\n",
        "    relev = [0] * k\r\n",
        "    for i, pred in enumerate(predictions):\r\n",
        "        if y_train[pred] == film:\r\n",
        "            relev[i] = 1\r\n",
        "    if relev != [0] * k:   \r\n",
        "        df = evaluation(query, relev, index, k)\r\n",
        "    else:\r\n",
        "        df = pd.DataFrame({'precision': 0.0,\r\n",
        "                           'average_precision': 0.0},\r\n",
        "                           index=[index])\r\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ_Heg8a3xN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08c7771b-5733-4daa-d7d8-bb9050ecd32c"
      },
      "source": [
        "model = gensim.models.KeyedVectors.load('/content/drive/MyDrive/Colab Notebooks/model.model') # tayga_none_fasttextcbow_300_10_2019\r\n",
        "\r\n",
        "for i, size in zip((16, 17, 18, 19, 20, 21, 22, 23),\r\n",
        "                   (100, 500, 837, 1000, 5000, 10000, 20000, 30000)):\r\n",
        "    f = 300\r\n",
        "    t = AnnoyIndex(f, 'angular')  # Length of item vector that will be indexed\r\n",
        "    y_train, x_train, y_test, x_test = open_data(size)\r\n",
        "    x_train = [re.findall('\\w+', text) for text in x_train]\r\n",
        "    x_test = [re.findall('\\w+', text) for text in x_test]\r\n",
        "\r\n",
        "    train_start = process_time()\r\n",
        "    for idx, text in enumerate(x_train):\r\n",
        "        vectors = []\r\n",
        "        for word in text:\r\n",
        "            try:\r\n",
        "                vectors.append(model.__getitem__(word))\r\n",
        "            except AttributeError:\r\n",
        "                continue\r\n",
        "        v = np.mean(vectors, axis=0)\r\n",
        "        try:\r\n",
        "            t.add_item(idx, v)\r\n",
        "        except ValueError:\r\n",
        "            continue\r\n",
        "    train_stop = process_time()\r\n",
        "    train_time = train_stop - train_start\r\n",
        "    t.build(100)  # 100 trees\r\n",
        "\r\n",
        "    df = pd.DataFrame(columns=['precision', 'average_precision'])\r\n",
        "    test_start = process_time()\r\n",
        "    for idx, (query, film) in enumerate(zip(x_test, y_test)):\r\n",
        "        vectors = []\r\n",
        "        for word in query:\r\n",
        "            try:\r\n",
        "                vectors.append(model.__getitem__(word))\r\n",
        "            except AttributeError:\r\n",
        "                continue\r\n",
        "        v = np.mean(vectors, axis=0)\r\n",
        "        top_10 = t.get_nns_by_vector(v, 10)  # find the 10 nearest neighbors\r\n",
        "        df2 = calc_map(query, film, top_10, 10, idx)\r\n",
        "        df = df.append(df2)\r\n",
        "    test_stop = process_time()\r\n",
        "    test_time = test_stop - test_start\r\n",
        "\r\n",
        "    print(f'done {size}')\r\n",
        "    df2 = pd.DataFrame({'embedding': 'Fasttext',\r\n",
        "                        'train size': len(y_train),\r\n",
        "                        'test size': len(y_test),\r\n",
        "                        'training time': round(train_time, 2),\r\n",
        "                        'inference time': round(test_time / len(y_test), 2),\r\n",
        "                        'recall': round(len([1 for prec in df.precision if prec != 0])/len(y_test), 4),\r\n",
        "                        'MAP': round(sum(df.average_precision)/len(y_test), 4)},\r\n",
        "                        index=[i])\r\n",
        "    MAP = MAP.append(df2)\r\n",
        "    MAP.to_csv('/content/drive/MyDrive/Colab Notebooks/MAP.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done 100\n",
            "done 500\n",
            "done 837\n",
            "done 1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "done 5000\n",
            "done 10000\n",
            "done 20000\n",
            "done 30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCAb9MzrthA9"
      },
      "source": [
        "# Word2Vec (pre-trained) & Locality Sensitive Hashing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJH_fhPYfYKa"
      },
      "source": [
        "def open_data_tag(size):\r\n",
        "    y_train, x_train, y_test, x_test = open(f'/content/drive/MyDrive/Colab Notebooks/data_format_tags/data_{size}.txt', encoding='utf-8').read().split('\\n&&&\\n')\r\n",
        "    y_train, x_train, y_test, x_test = y_train.split('\\n'), x_train.split('\\n'), y_test.split('\\n'), x_test.split('\\n')\r\n",
        "\r\n",
        "    for idx, text in enumerate(x_train):\r\n",
        "        x_train[idx] = [word for word in text.split() if word in model.vocab]\r\n",
        "\r\n",
        "    for idx, text in enumerate(x_test):\r\n",
        "        x_test[idx] = [word for word in text.split() if word in model.vocab]\r\n",
        "\r\n",
        "    return y_train, x_train, y_test, x_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GSVnCIz0m8n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bb21af2-2bc9-4692-e885-751a2f54d890"
      },
      "source": [
        "model = open('/content/drive/MyDrive/Colab Notebooks/model.bin', 'rb') # ruwikiruscorpora_upos_skipgram_300_2_2019\r\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(model, binary=True)\r\n",
        "\r\n",
        "for i, size in zip((24, 25, 26, 27, 28, 29, 30, 31),\r\n",
        "                   (100, 500, 837, 1000, 5000, 10000, 20000, 30000)):\r\n",
        "    f = 300\r\n",
        "    t = AnnoyIndex(f, 'angular')  # Length of item vector that will be indexed\r\n",
        "    y_train, x_train, y_test, x_test = open_data_tag(size)\r\n",
        "    train_start = process_time()\r\n",
        "    for idx, text in enumerate(x_train):\r\n",
        "        try:\r\n",
        "            v = np.mean(model[text], axis=0)\r\n",
        "        except ValueError:\r\n",
        "            continue\r\n",
        "        else:\r\n",
        "            t.add_item(idx, v)\r\n",
        "    train_stop = process_time()\r\n",
        "    train_time = train_stop - train_start\r\n",
        "    t.build(100)  # 100 trees\r\n",
        "\r\n",
        "    df = pd.DataFrame(columns=['precision', 'average_precision'])\r\n",
        "    test_start = process_time()\r\n",
        "    for idx, (query, film) in enumerate(zip(x_test, y_test)):\r\n",
        "        v = np.mean(model[query], axis=0)\r\n",
        "        top_10 = t.get_nns_by_vector(v, 10) # find the 10 nearest neighbors\r\n",
        "        df2 = calc_map(query, film, top_10, 10, idx)\r\n",
        "        df = df.append(df2)\r\n",
        "    test_stop = process_time()\r\n",
        "    test_time = test_stop - test_start\r\n",
        "\r\n",
        "    print(f'done {size}')\r\n",
        "    df2 = pd.DataFrame({'embedding': 'Word2Vec',\r\n",
        "                        'train size': len(y_train),\r\n",
        "                        'test size': len(y_test),\r\n",
        "                        'training time': round(train_time, 2),\r\n",
        "                        'inference time': round(test_time / len(y_test), 2),\r\n",
        "                        'recall': round(len([1 for prec in df.precision if prec != 0])/len(y_test), 4),\r\n",
        "                        'MAP': round(sum(df.average_precision)/len(y_test), 4)},\r\n",
        "                        index=[i])\r\n",
        "    MAP = MAP.append(df2)\r\n",
        "    MAP.to_csv('/content/drive/MyDrive/Colab Notebooks/MAP.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done 100\n",
            "done 500\n",
            "done 837\n",
            "done 1000\n",
            "done 5000\n",
            "done 10000\n",
            "done 20000\n",
            "done 30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4goZkk-l4q2"
      },
      "source": [
        "# Doc2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdWlgz64l4q5",
        "scrolled": true
      },
      "source": [
        "def feature_constructing(x_train, y_train):\n",
        "    tagged_data = []\n",
        "    for text, film  in zip(x_train, y_train):\n",
        "        try:\n",
        "            tagged_data.append(TaggedDocument(words=text, tags=[y_train.index(film)]))\n",
        "        except AttributeError:\n",
        "            continue\n",
        "    return tagged_data\n",
        "\n",
        "\n",
        "def doc2vec_retrieval(y_train, y_test, x_test):\n",
        "    df = pd.DataFrame(columns=['precision', 'average_precision'])\n",
        "    for index, query in enumerate(x_test):\n",
        "        test_data = [word for word in re.findall(r\"\\w+\", query.lower()) if word not in stops]\n",
        "        v1 = model.infer_vector(test_data)\n",
        "        similar_doc = model.docvecs.most_similar(positive=[v1], topn=10)\n",
        "        pred = []\n",
        "        for film, q in similar_doc:\n",
        "            pred.append([q, y_train[int(film)]])\n",
        "        relev = [0] * 10\n",
        "        for i in range(10):\n",
        "            if pred[i][1] == y_test[index]:\n",
        "                relev[i] = 1\n",
        "        if relev != [0] * 10:\n",
        "            df = df.append(evaluation(query, relev, index))\n",
        "        else:\n",
        "            df2 = pd.DataFrame({'precision': 0.0,\n",
        "                                'average_precision': 0.0},\n",
        "                                index=[index])\n",
        "            df = df.append(df2)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg5UQBGqC9L3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0086230b-3401-4a36-881d-bc9e5c14e179"
      },
      "source": [
        "max_epochs = 10\r\n",
        "vec_size = 50\r\n",
        "\r\n",
        "for i, size in zip((32, 33, 34, 35, 36, 37, 38, 39),\r\n",
        "                   (100, 500, 837, 1000, 5000, 10000, 20000, 30000)):\r\n",
        "    model = Doc2Vec(vector_size=vec_size,\r\n",
        "                    min_count=0.1,\r\n",
        "                    epochs=max_epochs)\r\n",
        "    y_train, x_train, y_test, x_test = open_data(size)\r\n",
        "    tagged_data = feature_constructing(x_train, y_train)\r\n",
        "    model.build_vocab(tagged_data)\r\n",
        "    train_start = process_time()\r\n",
        "    model.train(tagged_data, \r\n",
        "                total_examples=model.corpus_count, \r\n",
        "                epochs=model.epochs)\r\n",
        "    train_stop = process_time()\r\n",
        "    train_time = train_stop - train_start\r\n",
        "\r\n",
        "    test_start = process_time()\r\n",
        "    df = doc2vec_retrieval(y_train, y_test, x_test)\r\n",
        "    test_stop = process_time()\r\n",
        "    test_time = test_stop - test_start\r\n",
        "    print(f'done {size}')\r\n",
        "    df2 = pd.DataFrame({'embedding': 'Doc2Vec',\r\n",
        "                        'train size': len(y_train),\r\n",
        "                        'test size': len(y_test),\r\n",
        "                        'training time': round(train_time, 2),\r\n",
        "                        'inference time': round(test_time / len(y_test), 2),\r\n",
        "                        'recall': round(len([1 for prec in df.precision if prec != 0])/len(y_test), 4),\r\n",
        "                        'MAP': round(sum(df.average_precision)/len(y_test), 4)},\r\n",
        "                        index=[i])\r\n",
        "    MAP = MAP.append(df2)\r\n",
        "    MAP.to_csv('/content/drive/MyDrive/Colab Notebooks/MAP.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done 100\n",
            "done 500\n",
            "done 837\n",
            "done 1000\n",
            "done 5000\n",
            "done 10000\n",
            "done 20000\n",
            "done 30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj9fcE0W3s4a"
      },
      "source": [
        "# Universal Sentence Encoder (pre-trained) & Locality Sensitive Hashing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zabi5rKn3yNh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "142b606c-add3-4267-be7e-9d5d23fe41ab"
      },
      "source": [
        "model = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3')\r\n",
        "\r\n",
        "for i, size in zip((46, 47),  # 40, 41, 42, 43, 44, 45, \r\n",
        "                   (20000, 30000)):  # 100, 500, 837, 1000, 5000, 10000, \r\n",
        "    f = 512\r\n",
        "    t = AnnoyIndex(f, 'angular')  # Length of item vector that will be indexed\r\n",
        "    y_train, x_train, y_test, x_test = open_data(size)\r\n",
        "    train_start = process_time()\r\n",
        "    for idx, text in enumerate(x_train):\r\n",
        "        v = model(text)[0]\r\n",
        "        try:\r\n",
        "            t.add_item(idx, v)\r\n",
        "        except ValueError:\r\n",
        "            continue\r\n",
        "    train_stop = process_time()\r\n",
        "    train_time = train_stop - train_start\r\n",
        "\r\n",
        "    t.build(100)  # 100 trees\r\n",
        "    df = pd.DataFrame(columns=['precision', 'average_precision'])\r\n",
        "\r\n",
        "    test_start = process_time()\r\n",
        "    for idx, (query, film) in enumerate(zip(x_test, y_test)):\r\n",
        "        v = model(query)[0]\r\n",
        "        top_10 = t.get_nns_by_vector(v, 10) # find the 10 nearest neighbors\r\n",
        "        df2 = calc_map(query, film, top_10, 10, idx)\r\n",
        "        df = df.append(df2)\r\n",
        "    test_stop = process_time()\r\n",
        "    test_time = test_stop - test_start\r\n",
        "\r\n",
        "    print(f'done {size}')\r\n",
        "    df2 = pd.DataFrame({'embedding': 'Universal Sentence Encoder',\r\n",
        "                        'train size': len(y_train),\r\n",
        "                        'test size': len(y_test),\r\n",
        "                        'training time': round(train_time, 2),\r\n",
        "                        'inference time': round(test_time / len(y_test), 2),\r\n",
        "                        'recall': round(len([1 for prec in df.precision if prec != 0])/len(y_test), 4),\r\n",
        "                        'MAP': round(sum(df.average_precision)/len(y_test), 4)},\r\n",
        "                        index=[i])\r\n",
        "    MAP = MAP.append(df2)\r\n",
        "    MAP.to_csv('/content/drive/MyDrive/Colab Notebooks/MAP.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done 20000\n",
            "done 30000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}