{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uz5VdSxEl4qi"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-298ff7b04cc7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTaggedDocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpymystem3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMystem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from pymystem3 import Mystem\n",
    "import re\n",
    "from scipy import spatial\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = open('russian.txt').read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gensim\n",
      "  Using cached gensim-3.8.3.tar.gz (23.4 MB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\merku\\appdata\\roaming\\python\\python39\\site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\merku\\appdata\\roaming\\python\\python39\\site-packages (from gensim) (1.6.0)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\merku\\appdata\\roaming\\python\\python39\\site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\merku\\appdata\\roaming\\python\\python39\\site-packages (from gensim) (4.1.0)\n",
      "Using legacy 'setup.py install' for gensim, since package 'wheel' is not installed.\n",
      "Installing collected packages: gensim\n",
      "    Running setup.py install for gensim: started\n",
      "    Running setup.py install for gensim: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'c:\\program files\\python39\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\merku\\\\AppData\\\\Local\\\\Temp\\\\pip-install-cb8c9vxm\\\\gensim\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\merku\\\\AppData\\\\Local\\\\Temp\\\\pip-install-cb8c9vxm\\\\gensim\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\merku\\AppData\\Local\\Temp\\pip-record-01ycs5wu\\install-record.txt' --single-version-externally-managed --user --prefix= --compile --install-headers 'C:\\Users\\merku\\AppData\\Roaming\\Python\\Python39\\Include\\gensim'\n",
      "         cwd: C:\\Users\\merku\\AppData\\Local\\Temp\\pip-install-cb8c9vxm\\gensim\\\n",
      "    Complete output (453 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.9\n",
      "    creating build\\lib.win-amd64-3.9\\gensim\n",
      "    copying gensim\\downloader.py -> build\\lib.win-amd64-3.9\\gensim\n",
      "    copying gensim\\interfaces.py -> build\\lib.win-amd64-3.9\\gensim\n",
      "    copying gensim\\matutils.py -> build\\lib.win-amd64-3.9\\gensim\n",
      "    copying gensim\\nosy.py -> build\\lib.win-amd64-3.9\\gensim\n",
      "    copying gensim\\utils.py -> build\\lib.win-amd64-3.9\\gensim\n",
      "    copying gensim\\__init__.py -> build\\lib.win-amd64-3.9\\gensim\n",
      "    creating build\\lib.win-amd64-3.9\\gensim\\corpora\n",
      "    copying gensim\\corpora\\bleicorpus.py -> build\\lib.win-amd64-3.9\\gensim\\corpora\n",
      "    copying gensim\\corpora\\csvcorpus.py -> build\\lib.win-amd64-3.9\\gensim\\corpora\n",
      "    copying gensim\\corpora\\dictionary.py -> build\\lib.win-amd64-3.9\\gensim\\corpora\n",
      "    copying gensim\\corpora\\hashdictionary.py -> build\\lib.win-amd64-3.9\\gensim\\corpora\n",
      "    copying gensim\\corpora\\indexedcorpus.py -> build\\lib.win-amd64-3.9\\gensim\\corpora\n",
      "    copying gensim\\corpora\\lowcorpus.py -> build\\lib.win-amd64-3.9\\gensim\\corpora\n",
      "    copying gensim\\corpora\\malletcorpus.py -> build\\lib.win-amd64-3.9\\gensim\\corpora\n",
      "    copying gensim\\corpora\\mmcorpus.py -> build\\lib.win-amd64-3.9\\gensim\\corpora\n",
      "    copying gensim\\corpora\\sharded_corpus.py -> build\\lib.win-amd64-3.9\\gensim\\corpora\n",
      "    copying gensim\\corpora\\svmlightcorpus.py -> build\\lib.win-amd64-3.9\\gensim\\corpora\n",
      "    copying gensim\\corpora\\textcorpus.py -> build\\lib.win-amd64-3.9\\gensim\\corpora\n",
      "    copying gensim\\corpora\\ucicorpus.py -> build\\lib.win-amd64-3.9\\gensim\\corpora\n",
      "    copying gensim\\corpora\\wikicorpus.py -> build\\lib.win-amd64-3.9\\gensim\\corpora\n",
      "    copying gensim\\corpora\\__init__.py -> build\\lib.win-amd64-3.9\\gensim\\corpora\n",
      "    creating build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\atmodel.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\basemodel.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\base_any2vec.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\callbacks.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\coherencemodel.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\doc2vec.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\fasttext.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\hdpmodel.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\keyedvectors.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\ldamodel.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\ldamulticore.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\ldaseqmodel.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\lda_dispatcher.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\lda_worker.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\logentropy_model.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\lsimodel.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\lsi_dispatcher.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\lsi_worker.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\nmf.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\normmodel.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\phrases.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\poincare.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\rpmodel.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\tfidfmodel.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\translation_matrix.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\utils_any2vec.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\word2vec.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\_fasttext_bin.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\__init__.py -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    creating build\\lib.win-amd64-3.9\\gensim\\parsing\n",
      "    copying gensim\\parsing\\porter.py -> build\\lib.win-amd64-3.9\\gensim\\parsing\n",
      "    copying gensim\\parsing\\preprocessing.py -> build\\lib.win-amd64-3.9\\gensim\\parsing\n",
      "    copying gensim\\parsing\\__init__.py -> build\\lib.win-amd64-3.9\\gensim\\parsing\n",
      "    creating build\\lib.win-amd64-3.9\\gensim\\scripts\n",
      "    copying gensim\\scripts\\glove2word2vec.py -> build\\lib.win-amd64-3.9\\gensim\\scripts\n",
      "    copying gensim\\scripts\\make_wiki.py -> build\\lib.win-amd64-3.9\\gensim\\scripts\n",
      "    copying gensim\\scripts\\make_wikicorpus.py -> build\\lib.win-amd64-3.9\\gensim\\scripts\n",
      "    copying gensim\\scripts\\make_wiki_lemma.py -> build\\lib.win-amd64-3.9\\gensim\\scripts\n",
      "    copying gensim\\scripts\\make_wiki_online.py -> build\\lib.win-amd64-3.9\\gensim\\scripts\n",
      "    copying gensim\\scripts\\make_wiki_online_lemma.py -> build\\lib.win-amd64-3.9\\gensim\\scripts\n",
      "    copying gensim\\scripts\\make_wiki_online_nodebug.py -> build\\lib.win-amd64-3.9\\gensim\\scripts\n",
      "    copying gensim\\scripts\\package_info.py -> build\\lib.win-amd64-3.9\\gensim\\scripts\n",
      "    copying gensim\\scripts\\segment_wiki.py -> build\\lib.win-amd64-3.9\\gensim\\scripts\n",
      "    copying gensim\\scripts\\word2vec2tensor.py -> build\\lib.win-amd64-3.9\\gensim\\scripts\n",
      "    copying gensim\\scripts\\word2vec_standalone.py -> build\\lib.win-amd64-3.9\\gensim\\scripts\n",
      "    copying gensim\\scripts\\__init__.py -> build\\lib.win-amd64-3.9\\gensim\\scripts\n",
      "    creating build\\lib.win-amd64-3.9\\gensim\\similarities\n",
      "    copying gensim\\similarities\\docsim.py -> build\\lib.win-amd64-3.9\\gensim\\similarities\n",
      "    copying gensim\\similarities\\index.py -> build\\lib.win-amd64-3.9\\gensim\\similarities\n",
      "    copying gensim\\similarities\\levenshtein.py -> build\\lib.win-amd64-3.9\\gensim\\similarities\n",
      "    copying gensim\\similarities\\nmslib.py -> build\\lib.win-amd64-3.9\\gensim\\similarities\n",
      "    copying gensim\\similarities\\termsim.py -> build\\lib.win-amd64-3.9\\gensim\\similarities\n",
      "    copying gensim\\similarities\\__init__.py -> build\\lib.win-amd64-3.9\\gensim\\similarities\n",
      "    creating build\\lib.win-amd64-3.9\\gensim\\sklearn_api\n",
      "    copying gensim\\sklearn_api\\atmodel.py -> build\\lib.win-amd64-3.9\\gensim\\sklearn_api\n",
      "    copying gensim\\sklearn_api\\d2vmodel.py -> build\\lib.win-amd64-3.9\\gensim\\sklearn_api\n",
      "    copying gensim\\sklearn_api\\ftmodel.py -> build\\lib.win-amd64-3.9\\gensim\\sklearn_api\n",
      "    copying gensim\\sklearn_api\\hdp.py -> build\\lib.win-amd64-3.9\\gensim\\sklearn_api\n",
      "    copying gensim\\sklearn_api\\ldamodel.py -> build\\lib.win-amd64-3.9\\gensim\\sklearn_api\n",
      "    copying gensim\\sklearn_api\\ldaseqmodel.py -> build\\lib.win-amd64-3.9\\gensim\\sklearn_api\n",
      "    copying gensim\\sklearn_api\\lsimodel.py -> build\\lib.win-amd64-3.9\\gensim\\sklearn_api\n",
      "    copying gensim\\sklearn_api\\phrases.py -> build\\lib.win-amd64-3.9\\gensim\\sklearn_api\n",
      "    copying gensim\\sklearn_api\\rpmodel.py -> build\\lib.win-amd64-3.9\\gensim\\sklearn_api\n",
      "    copying gensim\\sklearn_api\\text2bow.py -> build\\lib.win-amd64-3.9\\gensim\\sklearn_api\n",
      "    copying gensim\\sklearn_api\\tfidf.py -> build\\lib.win-amd64-3.9\\gensim\\sklearn_api\n",
      "    copying gensim\\sklearn_api\\w2vmodel.py -> build\\lib.win-amd64-3.9\\gensim\\sklearn_api\n",
      "    copying gensim\\sklearn_api\\__init__.py -> build\\lib.win-amd64-3.9\\gensim\\sklearn_api\n",
      "    creating build\\lib.win-amd64-3.9\\gensim\\summarization\n",
      "    copying gensim\\summarization\\bm25.py -> build\\lib.win-amd64-3.9\\gensim\\summarization\n",
      "    copying gensim\\summarization\\commons.py -> build\\lib.win-amd64-3.9\\gensim\\summarization\n",
      "    copying gensim\\summarization\\graph.py -> build\\lib.win-amd64-3.9\\gensim\\summarization\n",
      "    copying gensim\\summarization\\keywords.py -> build\\lib.win-amd64-3.9\\gensim\\summarization\n",
      "    copying gensim\\summarization\\mz_entropy.py -> build\\lib.win-amd64-3.9\\gensim\\summarization\n",
      "    copying gensim\\summarization\\pagerank_weighted.py -> build\\lib.win-amd64-3.9\\gensim\\summarization\n",
      "    copying gensim\\summarization\\summarizer.py -> build\\lib.win-amd64-3.9\\gensim\\summarization\n",
      "    copying gensim\\summarization\\syntactic_unit.py -> build\\lib.win-amd64-3.9\\gensim\\summarization\n",
      "    copying gensim\\summarization\\textcleaner.py -> build\\lib.win-amd64-3.9\\gensim\\summarization\n",
      "    copying gensim\\summarization\\__init__.py -> build\\lib.win-amd64-3.9\\gensim\\summarization\n",
      "    creating build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\basetmtests.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\simspeed.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\simspeed2.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\svd_error.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_aggregation.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_api.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_atmodel.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_big.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_BM25.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_coherencemodel.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_corpora.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_corpora_dictionary.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_corpora_hashdictionary.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_d2vmodel.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_datatype.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_direct_confirmation.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_doc2vec.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_dtm.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_fasttext.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_fasttext_wrapper.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_glove2word2vec.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_hdpmodel.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_indirect_confirmation.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_keras_integration.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_keyedvectors.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_keywords.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_ldamallet_wrapper.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_ldamodel.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_ldaseqmodel.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_ldavowpalwabbit_wrapper.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_lda_callback.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_lee.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_logentropy_model.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_lsimodel.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_matutils.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_miislita.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_nmf.py -> build\\lib.win-amd64-3.9\\gensim\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    copying gensim\\test\\test_normmodel.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_parsing.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_phrases.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_poincare.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_probability_estimation.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_rpmodel.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_scripts.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_segmentation.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_sharded_corpus.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_similarities.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_similarity_metrics.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_sklearn_api.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_summarization.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_text_analysis.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_tfidfmodel.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_tmdiff.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_translation_matrix.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_utils.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_utils_any2vec.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_varembed_wrapper.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_word2vec.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\test_wordrank_wrapper.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\utils.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    copying gensim\\test\\__init__.py -> build\\lib.win-amd64-3.9\\gensim\\test\n",
      "    creating build\\lib.win-amd64-3.9\\gensim\\topic_coherence\n",
      "    copying gensim\\topic_coherence\\aggregation.py -> build\\lib.win-amd64-3.9\\gensim\\topic_coherence\n",
      "    copying gensim\\topic_coherence\\direct_confirmation_measure.py -> build\\lib.win-amd64-3.9\\gensim\\topic_coherence\n",
      "    copying gensim\\topic_coherence\\indirect_confirmation_measure.py -> build\\lib.win-amd64-3.9\\gensim\\topic_coherence\n",
      "    copying gensim\\topic_coherence\\probability_estimation.py -> build\\lib.win-amd64-3.9\\gensim\\topic_coherence\n",
      "    copying gensim\\topic_coherence\\segmentation.py -> build\\lib.win-amd64-3.9\\gensim\\topic_coherence\n",
      "    copying gensim\\topic_coherence\\text_analysis.py -> build\\lib.win-amd64-3.9\\gensim\\topic_coherence\n",
      "    copying gensim\\topic_coherence\\__init__.py -> build\\lib.win-amd64-3.9\\gensim\\topic_coherence\n",
      "    creating build\\lib.win-amd64-3.9\\gensim\\viz\n",
      "    copying gensim\\viz\\poincare.py -> build\\lib.win-amd64-3.9\\gensim\\viz\n",
      "    copying gensim\\viz\\__init__.py -> build\\lib.win-amd64-3.9\\gensim\\viz\n",
      "    creating build\\lib.win-amd64-3.9\\gensim\\models\\deprecated\n",
      "    copying gensim\\models\\deprecated\\doc2vec.py -> build\\lib.win-amd64-3.9\\gensim\\models\\deprecated\n",
      "    copying gensim\\models\\deprecated\\fasttext.py -> build\\lib.win-amd64-3.9\\gensim\\models\\deprecated\n",
      "    copying gensim\\models\\deprecated\\fasttext_wrapper.py -> build\\lib.win-amd64-3.9\\gensim\\models\\deprecated\n",
      "    copying gensim\\models\\deprecated\\keyedvectors.py -> build\\lib.win-amd64-3.9\\gensim\\models\\deprecated\n",
      "    copying gensim\\models\\deprecated\\old_saveload.py -> build\\lib.win-amd64-3.9\\gensim\\models\\deprecated\n",
      "    copying gensim\\models\\deprecated\\word2vec.py -> build\\lib.win-amd64-3.9\\gensim\\models\\deprecated\n",
      "    copying gensim\\models\\deprecated\\__init__.py -> build\\lib.win-amd64-3.9\\gensim\\models\\deprecated\n",
      "    creating build\\lib.win-amd64-3.9\\gensim\\models\\wrappers\n",
      "    copying gensim\\models\\wrappers\\dtmmodel.py -> build\\lib.win-amd64-3.9\\gensim\\models\\wrappers\n",
      "    copying gensim\\models\\wrappers\\fasttext.py -> build\\lib.win-amd64-3.9\\gensim\\models\\wrappers\n",
      "    copying gensim\\models\\wrappers\\ldamallet.py -> build\\lib.win-amd64-3.9\\gensim\\models\\wrappers\n",
      "    copying gensim\\models\\wrappers\\ldavowpalwabbit.py -> build\\lib.win-amd64-3.9\\gensim\\models\\wrappers\n",
      "    copying gensim\\models\\wrappers\\varembed.py -> build\\lib.win-amd64-3.9\\gensim\\models\\wrappers\n",
      "    copying gensim\\models\\wrappers\\wordrank.py -> build\\lib.win-amd64-3.9\\gensim\\models\\wrappers\n",
      "    copying gensim\\models\\wrappers\\__init__.py -> build\\lib.win-amd64-3.9\\gensim\\models\\wrappers\n",
      "    running egg_info\n",
      "    writing gensim.egg-info\\PKG-INFO\n",
      "    writing dependency_links to gensim.egg-info\\dependency_links.txt\n",
      "    writing requirements to gensim.egg-info\\requires.txt\n",
      "    writing top-level names to gensim.egg-info\\top_level.txt\n",
      "    reading manifest file 'gensim.egg-info\\SOURCES.txt'\n",
      "    reading manifest template 'MANIFEST.in'\n",
      "    warning: no files found matching 'COPYING.LESSER'\n",
      "    warning: no files found matching 'ez_setup.py'\n",
      "    warning: no files found matching 'gensim\\models\\doc2vec_inner.c'\n",
      "    writing manifest file 'gensim.egg-info\\SOURCES.txt'\n",
      "    copying gensim\\_matutils.c -> build\\lib.win-amd64-3.9\\gensim\n",
      "    copying gensim\\_matutils.pyx -> build\\lib.win-amd64-3.9\\gensim\n",
      "    copying gensim\\corpora\\_mmreader.c -> build\\lib.win-amd64-3.9\\gensim\\corpora\n",
      "    copying gensim\\models\\_utils_any2vec.c -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\doc2vec_corpusfile.cpp -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\doc2vec_inner.cpp -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\fasttext_corpusfile.cpp -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\fasttext_inner.c -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\nmf_pgd.c -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\word2vec_corpusfile.cpp -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\word2vec_inner.c -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\corpora\\_mmreader.pyx -> build\\lib.win-amd64-3.9\\gensim\\corpora\n",
      "    copying gensim\\models\\_utils_any2vec.pyx -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\doc2vec_corpusfile.pyx -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\doc2vec_inner.pxd -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\doc2vec_inner.pyx -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\fast_line_sentence.h -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\fasttext_corpusfile.pyx -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\fasttext_inner.pxd -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\fasttext_inner.pyx -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\nmf_pgd.pyx -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\stdint_wrapper.h -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\voidptr.h -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\word2vec_corpusfile.pxd -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\word2vec_corpusfile.pyx -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\word2vec_inner.pxd -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    copying gensim\\models\\word2vec_inner.pyx -> build\\lib.win-amd64-3.9\\gensim\\models\n",
      "    creating build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\IT.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\OPUS_en_it_europarl_train_one2ten.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\alldata-id-10.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\atmodel_3_0_1_model -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\atmodel_3_0_1_model.expElogbeta.npy -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\atmodel_3_0_1_model.id2word -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\atmodel_3_0_1_model.state -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\bgwiki-latest-pages-articles-shortened.xml.bz2 -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\compatible-hash-false.model -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\compatible-hash-true.model -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\cp852_fasttext.bin -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\crime-and-punishment.bin -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\crime-and-punishment.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\crime-and-punishment.vec -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\d2v-lee-v0.13.0 -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\doc2vec_old -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\doc2vec_old_sep -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\doc2vec_old_sep.syn0_lockf.npy -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\doc2vec_old_sep.syn1neg.npy -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    copying gensim\\test\\test_data\\dtm_test.dict -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\dtm_test.mm -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\enwiki-latest-pages-articles1.xml-p000000010p000030302-shortened.bz2 -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\enwiki-table-markup.xml.bz2 -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\euclidean_vectors.bin -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\fasttext_old -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\fasttext_old_sep -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\fasttext_old_sep.syn0_lockf.npy -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\fasttext_old_sep.syn1neg.npy -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\fb-ngrams.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\ft_kv_3.6.0.model.gz -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\ft_model_2.3.0 -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\head500.noblanks.cor -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\head500.noblanks.cor.bz2 -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\head500.noblanks.cor_tfidf.model -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\head500.noblanks.cor_wordids.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\high_precision.kv.bin -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\high_precision.kv.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\large_tag_doc_10_iter50 -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\lda_3_0_1_model -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\lda_3_0_1_model.expElogbeta.npy -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\lda_3_0_1_model.id2word -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\lda_3_0_1_model.state -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\ldamodel_python_2_7 -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\ldamodel_python_2_7.expElogbeta.npy -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\ldamodel_python_2_7.id2word -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\ldamodel_python_2_7.state -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\ldamodel_python_3_5 -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\ldamodel_python_3_5.expElogbeta.npy -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\ldamodel_python_3_5.id2word -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\ldamodel_python_3_5.state -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\ldavowpalwabbit.dict.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\ldavowpalwabbit.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\lee.cor -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\lee_background.cor -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\lee_fasttext -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\lee_fasttext.bin -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\lee_fasttext.vec -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\lee_fasttext_new.bin -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\miIslita.cor -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\mihalcea_tarau.kw.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\mihalcea_tarau.kwpos.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\mihalcea_tarau.summ.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\mihalcea_tarau.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\mini_newsgroup -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\nmf_model -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\non_ascii_fasttext.bin -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\old_keyedvectors_320.dat -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\pang_lee_polarity.cor -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\pang_lee_polarity_fasttext.bin -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\pang_lee_polarity_fasttext.vec -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\para2para_text1.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\para2para_text2.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\phraser-3.6.0.model -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\phraser-no-common-terms.pkl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\phraser-no-scoring.pkl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\phraser-scoring-str.pkl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\phrases-3.6.0.model -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\phrases-no-common-terms.pkl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\phrases-no-scoring.pkl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\phrases-scoring-str.pkl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\phrases-transformer-new-v3-5-0.pkl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\phrases-transformer-v3-5-0.pkl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\poincare_cp852.tsv -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\poincare_hypernyms.tsv -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\poincare_hypernyms_large.tsv -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\poincare_test_3.4.0 -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\poincare_utf8.tsv -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\poincare_vectors.bin -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\pre_0_13_2_model -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\pre_0_13_2_model.state -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\pretrained.vec -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\questions-words.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\reproduce.dat -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\reproduce.dat.gz -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\similarities0-1.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\simlex999.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\small_tag_doc_5_iter50 -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\test_corpus_ok.mm -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\test_corpus_small.mm -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\test_glove.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\test_mmcorpus_corrupt.mm -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\test_mmcorpus_no_index.mm -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\test_mmcorpus_no_index.mm.bz2 -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\test_mmcorpus_no_index.mm.gz -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\test_mmcorpus_overflow.mm -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\test_mmcorpus_with_index.mm -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\test_mmcorpus_with_index.mm.index -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\testcorpus.blei -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\testcorpus.blei.index -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\testcorpus.blei.vocab -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\testcorpus.low -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\testcorpus.low.index -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\testcorpus.mallet -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\testcorpus.mallet.index -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\testcorpus.mm -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\testcorpus.mm.index -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\testcorpus.svmlight -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\testcorpus.svmlight.index -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\testcorpus.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\testcorpus.uci -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\testcorpus.uci.index -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\testcorpus.uci.vocab -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\testcorpus.xml.bz2 -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\testcorpus_serialization.mm.index -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\testlowdistinctwords.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\testrepeatedkeywords.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\testsummarization_unrelated.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\tfidf_model.tst -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\tfidf_model.tst.bz2 -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\tfidf_model_3_2.tst -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\toy-data.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\toy-model-pretrained.bin -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\toy-model.bin -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\toy-model.vec -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\varembed_lee_subcorpus.cor -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\varembed_morfessor.bin -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\varembed_vectors.pkl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\w2v-lee-v0.12.0 -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\w2v_keyedvectors_load_test.modeldata -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\w2v_keyedvectors_load_test.vocab -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_3.3 -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_old -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_old_sep -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_old_sep.syn0_lockf.npy -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_old_sep.syn1neg.npy -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_pre_kv_c -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_pre_kv_py2 -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_pre_kv_py3 -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_pre_kv_py3_4 -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py2 -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py2.neg_labels.npy -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py2.syn0.npy -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py2.syn0_lockf.npy -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py2.syn1neg.npy -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3 -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3.neg_labels.npy -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3.syn0.npy -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3.syn0_lockf.npy -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3.syn1neg.npy -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3_4 -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3_4.neg_labels.npy -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3_4.syn0.npy -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3_4.syn0_lockf.npy -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\word2vec_pre_kv_sep_py3_4.syn1neg.npy -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    copying gensim\\test\\test_data\\wordsim353.tsv -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\n",
      "    creating build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\DTM\n",
      "    copying gensim\\test\\test_data\\DTM\\ldaseq_3_0_1_model -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\DTM\n",
      "    copying gensim\\test\\test_data\\DTM\\sstats_test.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\DTM\n",
      "    creating build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\PathLineSentences\n",
      "    copying gensim\\test\\test_data\\PathLineSentences\\1.txt -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\PathLineSentences\n",
      "    copying gensim\\test\\test_data\\PathLineSentences\\2.txt.bz2 -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\PathLineSentences\n",
      "    creating build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_d2v_models\n",
      "    copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.12.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_d2v_models\n",
      "    copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.12.1.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_d2v_models\n",
      "    copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.12.2.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_d2v_models\n",
      "    copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.12.3.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_d2v_models\n",
      "    copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.12.4.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_d2v_models\n",
      "    copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.13.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_d2v_models\n",
      "    copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.13.1.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_d2v_models\n",
      "    copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.13.2.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_d2v_models\n",
      "    copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.13.3.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_d2v_models\n",
      "    copying gensim\\test\\test_data\\old_d2v_models\\d2v_0.13.4.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_d2v_models\n",
      "    copying gensim\\test\\test_data\\old_d2v_models\\d2v_1.0.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_d2v_models\n",
      "    copying gensim\\test\\test_data\\old_d2v_models\\d2v_1.0.1.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_d2v_models\n",
      "    copying gensim\\test\\test_data\\old_d2v_models\\d2v_2.0.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_d2v_models\n",
      "    copying gensim\\test\\test_data\\old_d2v_models\\d2v_2.1.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_d2v_models\n",
      "    copying gensim\\test\\test_data\\old_d2v_models\\d2v_2.2.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_d2v_models\n",
      "    copying gensim\\test\\test_data\\old_d2v_models\\d2v_2.3.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_d2v_models\n",
      "    copying gensim\\test\\test_data\\old_d2v_models\\d2v_3.0.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_d2v_models\n",
      "    copying gensim\\test\\test_data\\old_d2v_models\\d2v_3.1.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_d2v_models\n",
      "    copying gensim\\test\\test_data\\old_d2v_models\\d2v_3.2.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_d2v_models\n",
      "    copying gensim\\test\\test_data\\old_d2v_models\\d2v_3.3.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_d2v_models\n",
      "    copying gensim\\test\\test_data\\old_d2v_models\\d2v_3.4.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_d2v_models\n",
      "    creating build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_w2v_models\n",
      "    copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.12.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_w2v_models\n",
      "    copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.12.1.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_w2v_models\n",
      "    copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.12.2.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_w2v_models\n",
      "    copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.12.3.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_w2v_models\n",
      "    copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.12.4.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_w2v_models\n",
      "    copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.13.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_w2v_models\n",
      "    copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.13.1.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_w2v_models\n",
      "    copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.13.2.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_w2v_models\n",
      "    copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.13.3.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_w2v_models\n",
      "    copying gensim\\test\\test_data\\old_w2v_models\\w2v_0.13.4.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_w2v_models\n",
      "    copying gensim\\test\\test_data\\old_w2v_models\\w2v_1.0.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_w2v_models\n",
      "    copying gensim\\test\\test_data\\old_w2v_models\\w2v_1.0.1.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_w2v_models\n",
      "    copying gensim\\test\\test_data\\old_w2v_models\\w2v_2.0.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_w2v_models\n",
      "    copying gensim\\test\\test_data\\old_w2v_models\\w2v_2.1.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_w2v_models\n",
      "    copying gensim\\test\\test_data\\old_w2v_models\\w2v_2.2.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_w2v_models\n",
      "    copying gensim\\test\\test_data\\old_w2v_models\\w2v_2.3.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_w2v_models\n",
      "    copying gensim\\test\\test_data\\old_w2v_models\\w2v_3.0.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_w2v_models\n",
      "    copying gensim\\test\\test_data\\old_w2v_models\\w2v_3.1.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_w2v_models\n",
      "    copying gensim\\test\\test_data\\old_w2v_models\\w2v_3.2.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_w2v_models\n",
      "    copying gensim\\test\\test_data\\old_w2v_models\\w2v_3.3.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_w2v_models\n",
      "    copying gensim\\test\\test_data\\old_w2v_models\\w2v_3.4.0.mdl -> build\\lib.win-amd64-3.9\\gensim\\test\\test_data\\old_w2v_models\n",
      "    running build_ext\n",
      "    building 'gensim.models.word2vec_inner' extension\n",
      "    error: Microsoft Visual C++ 14.0 is required. Get it with \"Build Tools for Visual Studio\": https://visualstudio.microsoft.com/downloads/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'c:\\program files\\python39\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\merku\\\\AppData\\\\Local\\\\Temp\\\\pip-install-cb8c9vxm\\\\gensim\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\merku\\\\AppData\\\\Local\\\\Temp\\\\pip-install-cb8c9vxm\\\\gensim\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\merku\\AppData\\Local\\Temp\\pip-record-01ycs5wu\\install-record.txt' --single-version-externally-managed --user --prefix= --compile --install-headers 'C:\\Users\\merku\\AppData\\Roaming\\Python\\Python39\\Include\\gensim' Check the logs for full command output.\n",
      "WARNING: You are using pip version 20.2.3; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the 'c:\\program files\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_O8a8yH9l4qk"
   },
   "source": [
    "#    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2rX7EXvQmGoF",
    "outputId": "9cc8b334-e862-4b2e-cf21-daae8cc125d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x_train = open('/content/drive/MyDrive/Colab Notebooks/film_plots_lemma.txt',\n",
    "               encoding='utf-8').read().split('\\n')\n",
    "\n",
    "y_train = open('/content/drive/MyDrive/Colab Notebooks/wiki_titles.txt',\n",
    "               encoding='utf-8').read().split('\\n')\n",
    "\n",
    "\n",
    "x_test = open('/content/drive/MyDrive/Colab Notebooks/test_data_lemma.txt',\n",
    "              encoding='utf-8').read().split('\\n')\n",
    "\n",
    "y_test = open('/content/drive/MyDrive/Colab Notebooks/test_titles.txt',\n",
    "              encoding='utf-8').read().split('\\n')\n",
    "\n",
    "x_summary_test = open('/content/drive/MyDrive/Colab Notebooks/summary_lemma.txt',\n",
    "                      encoding='utf-8').read().split('\\n')\n",
    "\n",
    "y_summary_test = open('/content/drive/MyDrive/Colab Notebooks/summary_titles.txt',\n",
    "                      encoding='utf-8').read().split('\\n')\n",
    "\n",
    "print(len(x_train) == len(y_train))\n",
    "print(len(x_test) == len(y_test))\n",
    "print(len(x_summary_test) == len(y_summary_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBxwkbvsl4qm"
   },
   "source": [
    "##     : preprocessing, embedding, accuracy\n",
    "\n",
    " -10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Z2WAGEUml4qn"
   },
   "outputs": [],
   "source": [
    "def ap(relev, k):    # average presicion\n",
    "    ap = []\n",
    "    for i in range(1, k):\n",
    "        if relev[i-1] is 1:    # if doc is relevant\n",
    "            ap.append(sum(relev[:i])/i)    # summary of precisions\n",
    "    try: \n",
    "        ap = sum(ap)/sum(relev)\n",
    "    except ZeroDivisionError:\n",
    "        ap = 0.0\n",
    "    return ap\n",
    "\n",
    "def evaluation(query, relev, index=0, k=10):    # mean average precision (10)\n",
    "    prec = round(sum(relev)/k, 4)    # rank is not take into account\n",
    "    k += 1\n",
    "    avp = ap(relev, k)    # rank is take into account\n",
    "    evaluat = pd.DataFrame({'query': query,\n",
    "                            'precision': prec,\n",
    "                            'average_precision': avp},\n",
    "                            index=[index])\n",
    "    return evaluat\n",
    "\n",
    "def retrieval(fit, predict):\n",
    "    df = pd.DataFrame(columns=['query', 'precision', 'average_precision'])\n",
    "    x = fit()\n",
    "    for index, query in enumerate(queries['query']):\n",
    "        predictions = predict(x, query)\n",
    "        relev = [0] * 10\n",
    "        for i, pred in enumerate(predictions):\n",
    "            if pred[1] == queries.film[index]:\n",
    "                relev[i] = 1\n",
    "        if relev != [0] * 10:\n",
    "            df = df.append(evaluation(query, relev, index))\n",
    "        else:\n",
    "            df2 = pd.DataFrame({'query': query,\n",
    "                                'precision': 0.0,\n",
    "                                'average_precision': 0.0},\n",
    "                                index=[index])\n",
    "            df = df.append(df2)\n",
    "    return df\n",
    "# map = sum(ap)/Q    # Q - number of quaries\n",
    "# recall = sum(p)/Q\n",
    "MAP = pd.DataFrame(columns=['preprocessing', 'embedding', 'recall', 'MAP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXOO6TZul4qo"
   },
   "source": [
    "# Baseline CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syY-5TRXl4qo",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# fitting\n",
    "def CV_fit():    \n",
    "    X = vectorizer.fit_transform(x_train)\n",
    "    return X.toarray()\n",
    "\n",
    "# similarity\n",
    "def CV_predict(x, query):\n",
    "    pred = []\n",
    "    vec = vectorizer.transform([query]).toarray()\n",
    "    simil = []\n",
    "    for vector, film in zip(x, y_train):\n",
    "        simil.append([1 - spatial.distance.cosine(vector, vec), film])\n",
    "    simil.sort(reverse=True)\n",
    "    for sim in simil[:10]:\n",
    "        pred.append(sim)\n",
    "    return pred\n",
    "\n",
    "df = retrieval(CV_fit, CV_predict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsDduLJ0l4qp",
    "outputId": "c6e70d4c-626f-4e91-f4b3-cad634c1ee70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    . -     .  ,       .      .         ,      .        ,       .        ,          - .         .      .   ,          .           -        ,     ,         -  .     ,      -  (     ,    )     -    ,  ,     .       ,          .     , -   .       ,       .        - ,        . -    ,  . .       ,   -      .     ,  -     ,     .       .           ,       ,   ,                (   cw).     ,            ,    ,   ,     ,      ,               .          ,     -  .   ,            ,   ,          ,         ,   -,        -.   ,         (   ,     ).               ,  -          ,    ,            .   ,        ,     ,  -    2     ,      ,        ,     ,   ,                   .  ,   30  ,      black lightning is back,                    ,                ,             .     ,      80,    ,                ,  ,      .       ,         ,         ,           .      ,          ,     .     ,      ,    cw    ,   ,   ,   .      ,        , ,      ,       .           ,      .      4  10   ,         .     ?  .                .               .             .   -    .          (       )   .      ,    ,        . .          + .          .     .       ,  ,            .  -     .         .       .    .                 . !       .        ,       .           .      . . '"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descrs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRabubhkl4qq",
    "outputId": "ba34d930-7be7-493a-c776-92d61512cbaf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>embedding</th>\n",
       "      <th>recall</th>\n",
       "      <th>MAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.134667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  preprocessing        embedding  recall       MAP\n",
       "0          None  CountVectorizer    0.22  0.134667"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame({'preprocessing': 'None',\n",
    "                    'embedding': 'CountVectorizer',\n",
    "                    'recall': len([1 for prec in df.precision if prec != 0])/len(queries),\n",
    "                    'MAP': sum(df.average_precision)/len(queries)},\n",
    "                    index=[0])\n",
    "MAP = MAP.append(df2)\n",
    "MAP.to_csv('MAP.csv')\n",
    "MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvqPmUHml4qq"
   },
   "source": [
    "### Improve CountVectorizer with\n",
    "- stop-words\n",
    "- lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3S3lXZUl4qq",
    "outputId": "edd46102-e775-49bb-f25a-e4ac3c0ad2fe",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>precision</th>\n",
       "      <th>average_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>  ,     ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,     </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>     ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>,  ,   ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>    ,     ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td> . ,   ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>     , ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>,       ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>,      ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>          ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>,    </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>   </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td> ,       ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>,  </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>      .</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>    </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>   </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>,   ,    </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>,      ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>,     , ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>  ,   , ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>,      </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>,       , ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>   </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>  </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>,      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>  90-   , ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>,       ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>     </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>,        ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>     , ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>   ,    ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>  ,   </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>,       ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>,        ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>      </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>     ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td> ,    ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>,      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td> ,     ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>  ,   ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>    ,   ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>   </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>     </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>      </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>   ,     ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>   </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>  it   </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>    </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>     </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>  ,   , </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>   ,  ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>    .  ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>    .  ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  precision  \\\n",
       "0         ...        0.0   \n",
       "1   ,      ...        0.0   \n",
       "2     ,     ...        0.2   \n",
       "3             ,             0.0   \n",
       "4        ...        0.2   \n",
       "5   ,  ,   ...        0.2   \n",
       "6       ,     ...        0.2   \n",
       "7    . ,   ...        0.2   \n",
       "8        , ...        0.2   \n",
       "9   ,       ...        0.0   \n",
       "10        ...        0.0   \n",
       "11  ,      ...        0.2   \n",
       "12        ...        0.2   \n",
       "13            ...        0.2   \n",
       "14                         ,            0.0   \n",
       "15                                   0.2   \n",
       "16   ,       ...        0.2   \n",
       "17                             ,          0.2   \n",
       "18              .        0.2   \n",
       "19                                 0.0   \n",
       "20                                     0.2   \n",
       "21  ,   ,            0.2   \n",
       "22  ,      ...        0.2   \n",
       "23  ,     , ...        0.2   \n",
       "24    ,   , ...        0.0   \n",
       "25           ,              0.2   \n",
       "26  ,       , ...        0.2   \n",
       "27                               0.2   \n",
       "28                                    0.2   \n",
       "29  ,      ...        0.0   \n",
       "..                                                ...        ...   \n",
       "70    90-   , ...        0.2   \n",
       "71  ,       ...        0.2   \n",
       "72                                0.0   \n",
       "73  ,        ...        0.0   \n",
       "74        ...        0.0   \n",
       "75       , ...        0.0   \n",
       "76     ,    ...        0.0   \n",
       "77          ,           0.0   \n",
       "78  ,       ...        0.2   \n",
       "79  ,        ...        0.2   \n",
       "80                       0.2   \n",
       "81       ...        0.2   \n",
       "82   ,    ...        0.0   \n",
       "83  ,      ...        0.0   \n",
       "84   ,     ...        0.2   \n",
       "85    ,   ...        0.2   \n",
       "86      ,   ...        0.0   \n",
       "87                                        0.0   \n",
       "88                           0.2   \n",
       "89                    0.0   \n",
       "90     ,     ...        0.0   \n",
       "91                                   0.0   \n",
       "92              it           0.0   \n",
       "93                     0.2   \n",
       "94        ...        0.0   \n",
       "95                                 0.0   \n",
       "96     ,   ,         0.2   \n",
       "97     ,  ...        0.0   \n",
       "98      .  ...        0.0   \n",
       "99      .  ...        0.0   \n",
       "\n",
       "    average_precision  \n",
       "0            0.000000  \n",
       "1            0.000000  \n",
       "2            0.500000  \n",
       "3            0.000000  \n",
       "4            1.000000  \n",
       "5            0.250000  \n",
       "6            0.250000  \n",
       "7            1.000000  \n",
       "8            0.500000  \n",
       "9            0.000000  \n",
       "10           0.000000  \n",
       "11           1.000000  \n",
       "12           0.500000  \n",
       "13           0.200000  \n",
       "14           0.000000  \n",
       "15           1.000000  \n",
       "16           1.000000  \n",
       "17           0.333333  \n",
       "18           1.000000  \n",
       "19           0.000000  \n",
       "20           1.000000  \n",
       "21           1.000000  \n",
       "22           0.500000  \n",
       "23           1.000000  \n",
       "24           0.000000  \n",
       "25           1.000000  \n",
       "26           1.000000  \n",
       "27           1.000000  \n",
       "28           0.250000  \n",
       "29           0.000000  \n",
       "..                ...  \n",
       "70           0.333333  \n",
       "71           1.000000  \n",
       "72           0.000000  \n",
       "73           0.000000  \n",
       "74           0.000000  \n",
       "75           0.000000  \n",
       "76           0.000000  \n",
       "77           0.000000  \n",
       "78           1.000000  \n",
       "79           1.000000  \n",
       "80           0.500000  \n",
       "81           1.000000  \n",
       "82           0.000000  \n",
       "83           0.000000  \n",
       "84           0.500000  \n",
       "85           0.500000  \n",
       "86           0.000000  \n",
       "87           0.000000  \n",
       "88           1.000000  \n",
       "89           0.000000  \n",
       "90           0.000000  \n",
       "91           0.000000  \n",
       "92           0.000000  \n",
       "93           1.000000  \n",
       "94           0.000000  \n",
       "95           0.000000  \n",
       "96           1.000000  \n",
       "97           0.000000  \n",
       "98           0.000000  \n",
       "99           0.000000  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_descrs = open('lemma_descriptions.txt').read().split('\\n&&&\\n')\n",
    "vectorizer = CountVectorizer(stop_words=stops)\n",
    "\n",
    "# fitting\n",
    "def CV_improved_fit():\n",
    "    X = vectorizer.fit_transform(lemma_descrs)\n",
    "    return X.toarray()\n",
    "\n",
    "# similarity\n",
    "def CV_improved_predict(x, query):\n",
    "    pred = []\n",
    "    query = ''.join(Mystem().lemmatize(query.lower()))\n",
    "    vec = vectorizer.transform([query]).toarray()\n",
    "    simil = []\n",
    "    for vector, film in zip(x, titles):\n",
    "        simil.append([1 - spatial.distance.cosine(vector, vec), film])\n",
    "    simil.sort(reverse=True)\n",
    "    for sim in simil[:5]:\n",
    "        pred.append(sim)\n",
    "    return pred\n",
    "\n",
    "df = retrieval(CV_improved_fit, CV_improved_predict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfG5mRlal4qr",
    "outputId": "0f47954f-21d0-466a-e11a-1eff9bffe923"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>embedding</th>\n",
       "      <th>recall</th>\n",
       "      <th>MAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.134667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stop-words, lemmatization</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.446333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               preprocessing        embedding  recall       MAP\n",
       "0                       None  CountVectorizer    0.22  0.134667\n",
       "1  stop-words, lemmatization  CountVectorizer    0.58  0.446333"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame({'preprocessing': 'stop-words, lemmatization',\n",
    "                    'embedding': 'CountVectorizer',\n",
    "                    'recall': len([1 for prec in df.precision if prec != 0])/len(queries),\n",
    "                    'MAP': sum(df.average_precision)/len(queries)},\n",
    "                    index=[1])\n",
    "MAP = MAP.append(df2)\n",
    "MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXeykSMYl4qs"
   },
   "source": [
    "#  split sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0l0-Af3Vl4qs",
    "outputId": "12893300-0f95-48b4-f370-4dd7e8aa3d1c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>precision</th>\n",
       "      <th>average_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>  ,     ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,     </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>     ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>,  ,   ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>    ,     ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td> . ,   ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>     , ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>,       ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>,      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>          ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>,    </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>   </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td> ,       ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>,  </td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>      .</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>    </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>   </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>,   ,    </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>,      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>,     , ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>  ,   , ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>,      </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>,       , ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>   </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>  </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>,      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>  90-   , ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>,       ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>     </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>,        ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>     , ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>   ,    ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>  ,   </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>,       ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>,        ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>      </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>     ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td> ,    ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>,      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td> ,     ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>  ,   ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>    ,   ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>   </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>     </td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>      </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>   ,     ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>   </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>  it   </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>    </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>     </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>  ,   , </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>   ,  ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>    .  ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>    .  ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  precision  \\\n",
       "0         ...        0.0   \n",
       "1   ,      ...        0.0   \n",
       "2     ,     ...        0.2   \n",
       "3             ,             0.0   \n",
       "4        ...        0.0   \n",
       "5   ,  ,   ...        0.0   \n",
       "6       ,     ...        0.0   \n",
       "7    . ,   ...        0.0   \n",
       "8        , ...        0.0   \n",
       "9   ,       ...        0.2   \n",
       "10        ...        0.0   \n",
       "11  ,      ...        0.0   \n",
       "12        ...        0.0   \n",
       "13            ...        0.2   \n",
       "14                         ,            0.0   \n",
       "15                                   0.0   \n",
       "16   ,       ...        0.0   \n",
       "17                             ,          0.4   \n",
       "18              .        0.4   \n",
       "19                                 0.0   \n",
       "20                                     0.2   \n",
       "21  ,   ,            0.0   \n",
       "22  ,      ...        0.0   \n",
       "23  ,     , ...        0.2   \n",
       "24    ,   , ...        0.0   \n",
       "25           ,              0.0   \n",
       "26  ,       , ...        0.2   \n",
       "27                               0.2   \n",
       "28                                    0.2   \n",
       "29  ,      ...        0.0   \n",
       "..                                                ...        ...   \n",
       "70    90-   , ...        0.0   \n",
       "71  ,       ...        0.0   \n",
       "72                                0.0   \n",
       "73  ,        ...        0.0   \n",
       "74        ...        0.0   \n",
       "75       , ...        0.2   \n",
       "76     ,    ...        0.0   \n",
       "77          ,           0.0   \n",
       "78  ,       ...        0.0   \n",
       "79  ,        ...        0.2   \n",
       "80                       0.0   \n",
       "81       ...        0.2   \n",
       "82   ,    ...        0.0   \n",
       "83  ,      ...        0.0   \n",
       "84   ,     ...        0.0   \n",
       "85    ,   ...        0.0   \n",
       "86      ,   ...        0.2   \n",
       "87                                        0.0   \n",
       "88                           0.4   \n",
       "89                    0.0   \n",
       "90     ,     ...        0.0   \n",
       "91                                   0.0   \n",
       "92              it           0.0   \n",
       "93                     0.0   \n",
       "94        ...        0.0   \n",
       "95                                 0.0   \n",
       "96     ,   ,         0.0   \n",
       "97     ,  ...        0.0   \n",
       "98      .  ...        0.0   \n",
       "99      .  ...        0.0   \n",
       "\n",
       "    average_precision  \n",
       "0            0.000000  \n",
       "1            0.000000  \n",
       "2            0.500000  \n",
       "3            0.000000  \n",
       "4            0.000000  \n",
       "5            0.000000  \n",
       "6            0.000000  \n",
       "7            0.000000  \n",
       "8            0.000000  \n",
       "9            0.500000  \n",
       "10           0.000000  \n",
       "11           0.000000  \n",
       "12           0.000000  \n",
       "13           1.000000  \n",
       "14           0.000000  \n",
       "15           0.000000  \n",
       "16           0.000000  \n",
       "17           0.700000  \n",
       "18           0.416667  \n",
       "19           0.000000  \n",
       "20           0.250000  \n",
       "21           0.000000  \n",
       "22           0.000000  \n",
       "23           1.000000  \n",
       "24           0.000000  \n",
       "25           0.000000  \n",
       "26           0.333333  \n",
       "27           1.000000  \n",
       "28           0.500000  \n",
       "29           0.000000  \n",
       "..                ...  \n",
       "70           0.000000  \n",
       "71           0.000000  \n",
       "72           0.000000  \n",
       "73           0.000000  \n",
       "74           0.000000  \n",
       "75           0.200000  \n",
       "76           0.000000  \n",
       "77           0.000000  \n",
       "78           0.000000  \n",
       "79           0.250000  \n",
       "80           0.000000  \n",
       "81           1.000000  \n",
       "82           0.000000  \n",
       "83           0.000000  \n",
       "84           0.000000  \n",
       "85           0.000000  \n",
       "86           0.500000  \n",
       "87           0.000000  \n",
       "88           0.366667  \n",
       "89           0.000000  \n",
       "90           0.000000  \n",
       "91           0.000000  \n",
       "92           0.000000  \n",
       "93           0.000000  \n",
       "94           0.000000  \n",
       "95           0.000000  \n",
       "96           0.000000  \n",
       "97           0.000000  \n",
       "98           0.000000  \n",
       "99           0.000000  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('lemma_films.csv')\n",
    "\n",
    "# fitting\n",
    "def CV_sentences_fit():\n",
    "    X = vectorizer.fit_transform(list(data.sentence))\n",
    "    return X\n",
    "\n",
    "# similarity\n",
    "def CV_sentences_predict(x, query):\n",
    "    pred = []\n",
    "    query = ''.join(Mystem().lemmatize(query.lower()))\n",
    "    vec = vectorizer.transform([query]).toarray()\n",
    "    simil = []\n",
    "    for vector, film, sent in zip(x, list(data.film), list(data.sentence)):\n",
    "        simil.append([1 - spatial.distance.cosine(vector.toarray(), vec), film, sent])\n",
    "    simil.sort(reverse=True)\n",
    "    for sim in simil[:5]:\n",
    "        pred.append(sim)\n",
    "    return pred\n",
    "\n",
    "df = retrieval(CV_sentences_fit, CV_sentences_predict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-rQsYjFl4qw",
    "outputId": "eb0eab53-7f3c-4bf4-843c-b99a462b6cc1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>embedding</th>\n",
       "      <th>recall</th>\n",
       "      <th>MAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.134667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stop-words, lemmatization</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.446333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>split_sentence, stop-words, lemmatization</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.138056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               preprocessing        embedding  recall  \\\n",
       "0                                       None  CountVectorizer    0.22   \n",
       "1                  stop-words, lemmatization  CountVectorizer    0.58   \n",
       "2  split_sentence, stop-words, lemmatization  CountVectorizer    0.24   \n",
       "\n",
       "        MAP  \n",
       "0  0.134667  \n",
       "1  0.446333  \n",
       "2  0.138056  "
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame({'preprocessing': 'split_sentence, stop-words, lemmatization',\n",
    "                    'embedding': 'CountVectorizer',\n",
    "                    'recall': len([1 for prec in df.precision if prec != 0])/len(queries),\n",
    "                    'MAP': sum(df.average_precision)/len(queries)},\n",
    "                    index=[2])\n",
    "MAP = MAP.append(df2)\n",
    "MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnacPTnRl4qw"
   },
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbveqjhyl4qx",
    "outputId": "3f75708b-5ac2-43ce-d87e-55603c6b5e98",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>precision</th>\n",
       "      <th>average_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,      ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>  ,     ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,     </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>     ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>,  ,   ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>    ,     ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td> . ,   ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>     , ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>,       ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>,      ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>          ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>,    </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>   </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td> ,       ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>,  </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>      .</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>    </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>   </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>,   ,    </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>,      ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>,     , ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>  ,   , ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>,      </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>,       , ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>   </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>  </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>,      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>  90-   , ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>,       ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>     </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>,        ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>     , ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>   ,    ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>  ,   </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>,       ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>,        ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>      </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>     ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td> ,    ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>,      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td> ,     ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>  ,   ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>    ,   ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>   </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>     </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>      </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>   ,     ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>   </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>  it   </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>    </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>     </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>  ,   , </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>   ,  ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>    .  ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>    .  ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  precision  \\\n",
       "0         ...        0.0   \n",
       "1   ,      ...        0.2   \n",
       "2     ,     ...        0.2   \n",
       "3             ,             0.0   \n",
       "4        ...        0.2   \n",
       "5   ,  ,   ...        0.2   \n",
       "6       ,     ...        0.0   \n",
       "7    . ,   ...        0.2   \n",
       "8        , ...        0.2   \n",
       "9   ,       ...        0.0   \n",
       "10        ...        0.2   \n",
       "11  ,      ...        0.2   \n",
       "12        ...        0.2   \n",
       "13            ...        0.2   \n",
       "14                         ,            0.2   \n",
       "15                                   0.0   \n",
       "16   ,       ...        0.2   \n",
       "17                             ,          0.2   \n",
       "18              .        0.2   \n",
       "19                                 0.0   \n",
       "20                                     0.2   \n",
       "21  ,   ,            0.2   \n",
       "22  ,      ...        0.2   \n",
       "23  ,     , ...        0.2   \n",
       "24    ,   , ...        0.0   \n",
       "25           ,              0.2   \n",
       "26  ,       , ...        0.2   \n",
       "27                               0.2   \n",
       "28                                    0.2   \n",
       "29  ,      ...        0.0   \n",
       "..                                                ...        ...   \n",
       "70    90-   , ...        0.2   \n",
       "71  ,       ...        0.2   \n",
       "72                                0.2   \n",
       "73  ,        ...        0.0   \n",
       "74        ...        0.0   \n",
       "75       , ...        0.2   \n",
       "76     ,    ...        0.0   \n",
       "77          ,           0.0   \n",
       "78  ,       ...        0.0   \n",
       "79  ,        ...        0.2   \n",
       "80                       0.0   \n",
       "81       ...        0.2   \n",
       "82   ,    ...        0.2   \n",
       "83  ,      ...        0.0   \n",
       "84   ,     ...        0.0   \n",
       "85    ,   ...        0.0   \n",
       "86      ,   ...        0.2   \n",
       "87                                        0.0   \n",
       "88                           0.0   \n",
       "89                    0.2   \n",
       "90     ,     ...        0.0   \n",
       "91                                   0.0   \n",
       "92              it           0.2   \n",
       "93                     0.2   \n",
       "94        ...        0.0   \n",
       "95                                 0.0   \n",
       "96     ,   ,         0.2   \n",
       "97     ,  ...        0.2   \n",
       "98      .  ...        0.2   \n",
       "99      .  ...        0.2   \n",
       "\n",
       "    average_precision  \n",
       "0            0.000000  \n",
       "1            0.333333  \n",
       "2            1.000000  \n",
       "3            0.000000  \n",
       "4            0.500000  \n",
       "5            1.000000  \n",
       "6            0.000000  \n",
       "7            1.000000  \n",
       "8            1.000000  \n",
       "9            0.000000  \n",
       "10           0.333333  \n",
       "11           1.000000  \n",
       "12           0.250000  \n",
       "13           0.200000  \n",
       "14           0.333333  \n",
       "15           0.000000  \n",
       "16           1.000000  \n",
       "17           1.000000  \n",
       "18           1.000000  \n",
       "19           0.000000  \n",
       "20           0.333333  \n",
       "21           1.000000  \n",
       "22           0.500000  \n",
       "23           1.000000  \n",
       "24           0.000000  \n",
       "25           0.500000  \n",
       "26           0.500000  \n",
       "27           1.000000  \n",
       "28           0.500000  \n",
       "29           0.000000  \n",
       "..                ...  \n",
       "70           0.500000  \n",
       "71           1.000000  \n",
       "72           0.500000  \n",
       "73           0.000000  \n",
       "74           0.000000  \n",
       "75           0.333333  \n",
       "76           0.000000  \n",
       "77           0.000000  \n",
       "78           0.000000  \n",
       "79           1.000000  \n",
       "80           0.000000  \n",
       "81           1.000000  \n",
       "82           1.000000  \n",
       "83           0.000000  \n",
       "84           0.000000  \n",
       "85           0.000000  \n",
       "86           0.250000  \n",
       "87           0.000000  \n",
       "88           0.000000  \n",
       "89           0.250000  \n",
       "90           0.000000  \n",
       "91           0.000000  \n",
       "92           1.000000  \n",
       "93           0.500000  \n",
       "94           0.000000  \n",
       "95           0.000000  \n",
       "96           0.200000  \n",
       "97           0.200000  \n",
       "98           1.000000  \n",
       "99           0.200000  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# fitting\n",
    "def tfidf_fit():\n",
    "    X = vectorizer.fit_transform(descrs)\n",
    "    return X.toarray()\n",
    "\n",
    "# similarity\n",
    "def tfidf_predict(x, query):\n",
    "    pred = []\n",
    "    vec = vectorizer.transform([query]).toarray()\n",
    "    simil = []\n",
    "    for vector, film in zip(x, titles):\n",
    "        simil.append([1 - spatial.distance.cosine(vector, vec), film])\n",
    "    simil.sort(reverse=True)\n",
    "    for sim in simil[:5]:\n",
    "        pred.append(sim)\n",
    "    return pred\n",
    "\n",
    "df = retrieval(tfidf_fit, tfidf_predict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33rXkAUGl4qx",
    "outputId": "75f8c659-d5bb-4c10-b1ea-6c1215d02fb2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>embedding</th>\n",
       "      <th>recall</th>\n",
       "      <th>MAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.134667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stop-words, lemmatization</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.446333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>split_sentence, stop-words, lemmatization</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.138056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.457833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               preprocessing        embedding  recall  \\\n",
       "0                                       None  CountVectorizer    0.22   \n",
       "1                  stop-words, lemmatization  CountVectorizer    0.58   \n",
       "2  split_sentence, stop-words, lemmatization  CountVectorizer    0.24   \n",
       "3                                       None           TF-IDF    0.67   \n",
       "\n",
       "        MAP  \n",
       "0  0.134667  \n",
       "1  0.446333  \n",
       "2  0.138056  \n",
       "3  0.457833  "
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame({'preprocessing': 'None',\n",
    "                    'embedding': 'TF-IDF',\n",
    "                    'recall': len([1 for prec in df.precision if prec != 0])/len(queries),\n",
    "                    'MAP': sum(df.average_precision)/len(queries)},\n",
    "                    index=[3])\n",
    "MAP = MAP.append(df2)\n",
    "MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_3t3gp0l4qy"
   },
   "source": [
    "### Improve tf-idf with\n",
    "- stop-words\n",
    "- lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MItVV0Fel4qy",
    "outputId": "d3bc7495-ce66-4a52-9496-7d560d14d73a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-8c0f23d41641>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mretrieval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_improved_fit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_improved_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-cf4222fcaeeb>\u001b[0m in \u001b[0;36mretrieval\u001b[1;34m(fit, predict)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'query'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mrelev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-8c0f23d41641>\u001b[0m in \u001b[0;36mtfidf_improved_predict\u001b[1;34m(x, query)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtfidf_improved_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mvec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0msimil\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pymystem3\\mystem.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[0mneed_encode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[0minfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lemma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pymystem3\\mystem.py\u001b[0m in \u001b[0;36manalyze\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_analyze_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pymystem3\\mystem.py\u001b[0m in \u001b[0;36m_analyze_impl\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_procin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_NL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m             \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_proc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_proc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[1;34m(self, input, timeout)\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 863\u001b[1;33m                 \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    864\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communication_started\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[1;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[0;32m   1110\u001b[0m             \u001b[1;31m# calls communicate again.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1112\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_remaining_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1113\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m             \u001b[1;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1070\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# already determined that the C code is done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1072\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lemma_descrs = open('lemma_descriptions.txt').read().split('\\n&&&\\n')\n",
    "vectorizer = TfidfVectorizer(stop_words=stops)\n",
    "\n",
    "# fitting\n",
    "def tfidf_improved_fit():\n",
    "    X = vectorizer.fit_transform(lemma_descrs)\n",
    "    return X.toarray()\n",
    "\n",
    "# similarity\n",
    "def tfidf_improved_predict(x, query):\n",
    "    pred = []\n",
    "    query = ''.join(Mystem().lemmatize(query.lower()))\n",
    "    vec = vectorizer.transform([query]).toarray()\n",
    "    simil = []\n",
    "    for vector, film in zip(x, titles):\n",
    "        simil.append([1 - spatial.distance.cosine(vector, vec), film])\n",
    "    simil.sort(reverse=True)\n",
    "    for sim in simil[:5]:\n",
    "        pred.append(sim)\n",
    "    v1 = vectorizer.transform([\"\"]).toarray()\n",
    "    v2 = vectorizer.transform([\"\"]).toarray()\n",
    "    cos_sim = 1 - spatial.distance.cosine(v1, v2)\n",
    "    print(cos_sim)\n",
    "    return pred\n",
    "\n",
    "df = retrieval(tfidf_improved_fit, tfidf_improved_predict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjYWl7EXl4qy",
    "outputId": "6a6ce619-018e-4782-8897-167867ffcf55"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>precision</th>\n",
       "      <th>average_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>    </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>   </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             query  precision  average_precision\n",
       "19              0.0                0.0\n",
       "20                  0.2                1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[19:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ng2xwA4Il4qz",
    "outputId": "3ee40b46-cf23-4c51-e5ad-dff57c8fc323"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>embedding</th>\n",
       "      <th>recall</th>\n",
       "      <th>MAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.134667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stop-words, lemmatization</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.446333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>split_sentence, stop-words, lemmatization</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.138056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.457833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stop-words, lemmatization</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.565833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               preprocessing        embedding  recall  \\\n",
       "0                                       None  CountVectorizer    0.22   \n",
       "1                  stop-words, lemmatization  CountVectorizer    0.58   \n",
       "2  split_sentence, stop-words, lemmatization  CountVectorizer    0.24   \n",
       "3                                       None           TF-IDF    0.67   \n",
       "4                  stop-words, lemmatization           TF-IDF    0.78   \n",
       "\n",
       "        MAP  \n",
       "0  0.134667  \n",
       "1  0.446333  \n",
       "2  0.138056  \n",
       "3  0.457833  \n",
       "4  0.565833  "
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame({'preprocessing': 'stop-words, lemmatization',\n",
    "                    'embedding': 'TF-IDF',\n",
    "                    'recall': len([1 for prec in df.precision if prec != 0])/len(queries),\n",
    "                    'MAP': sum(df.average_precision)/len(queries)},\n",
    "                    index=[4])\n",
    "MAP = MAP.append(df2)\n",
    "MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBAB6iazl4q0"
   },
   "source": [
    "# split sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qN_YKwfFl4q1",
    "outputId": "54fa127c-1249-4019-f185-789dc435ec87",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>precision</th>\n",
       "      <th>average_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,      ...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>  ,     ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,     </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>     ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>,  ,   ...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>    ,     ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td> . ,   ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>     , ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>,       ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>,      ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>          ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>,    </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>   </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td> ,       ...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>,  </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>      .</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>    </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>   </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>,   ,    </td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.477778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>,      ...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>,     , ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>  ,   , ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>,      </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>,       , ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>   </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>  </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>,      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>  90-   , ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>,       ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>     </td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.638889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>,        ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>     , ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>   ,    ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>  ,   </td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.755556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>,       ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>,        ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>      </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>     ...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td> ,    ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>,      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td> ,     ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>  ,   ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>    ,   ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>   </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>     </td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.804167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>      </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>   ,     ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>   </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>  it   </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>    </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>     </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>  ,   , </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>   ,  ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>    .  ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>    .  ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  precision  \\\n",
       "0         ...        0.0   \n",
       "1   ,      ...        0.6   \n",
       "2     ,     ...        0.2   \n",
       "3             ,             0.0   \n",
       "4        ...        0.2   \n",
       "5   ,  ,   ...        0.6   \n",
       "6       ,     ...        0.0   \n",
       "7    . ,   ...        0.0   \n",
       "8        , ...        0.0   \n",
       "9   ,       ...        0.2   \n",
       "10        ...        0.0   \n",
       "11  ,      ...        0.2   \n",
       "12        ...        0.2   \n",
       "13            ...        0.2   \n",
       "14                         ,            0.0   \n",
       "15                                   0.0   \n",
       "16   ,       ...        0.4   \n",
       "17                             ,          0.2   \n",
       "18              .        0.2   \n",
       "19                                 0.0   \n",
       "20                                     0.2   \n",
       "21  ,   ,            0.6   \n",
       "22  ,      ...        0.6   \n",
       "23  ,     , ...        0.0   \n",
       "24    ,   , ...        0.0   \n",
       "25           ,              0.0   \n",
       "26  ,       , ...        0.2   \n",
       "27                               0.2   \n",
       "28                                    0.0   \n",
       "29  ,      ...        0.0   \n",
       "..                                                ...        ...   \n",
       "70    90-   , ...        0.0   \n",
       "71  ,       ...        0.0   \n",
       "72                                0.6   \n",
       "73  ,        ...        0.0   \n",
       "74        ...        0.0   \n",
       "75       , ...        0.2   \n",
       "76     ,    ...        0.0   \n",
       "77          ,           0.6   \n",
       "78  ,       ...        0.0   \n",
       "79  ,        ...        0.2   \n",
       "80                       0.2   \n",
       "81       ...        0.4   \n",
       "82   ,    ...        0.2   \n",
       "83  ,      ...        0.0   \n",
       "84   ,     ...        0.0   \n",
       "85    ,   ...        0.0   \n",
       "86      ,   ...        0.2   \n",
       "87                                        0.2   \n",
       "88                           0.8   \n",
       "89                    0.0   \n",
       "90     ,     ...        0.0   \n",
       "91                                   0.2   \n",
       "92              it           0.0   \n",
       "93                     0.2   \n",
       "94        ...        0.0   \n",
       "95                                 0.0   \n",
       "96     ,   ,         0.0   \n",
       "97     ,  ...        0.0   \n",
       "98      .  ...        0.0   \n",
       "99      .  ...        0.0   \n",
       "\n",
       "    average_precision  \n",
       "0            0.000000  \n",
       "1            1.000000  \n",
       "2            1.000000  \n",
       "3            0.000000  \n",
       "4            1.000000  \n",
       "5            0.916667  \n",
       "6            0.000000  \n",
       "7            0.000000  \n",
       "8            0.000000  \n",
       "9            0.500000  \n",
       "10           0.000000  \n",
       "11           0.200000  \n",
       "12           0.500000  \n",
       "13           1.000000  \n",
       "14           0.000000  \n",
       "15           0.000000  \n",
       "16           0.416667  \n",
       "17           0.200000  \n",
       "18           0.333333  \n",
       "19           0.000000  \n",
       "20           0.200000  \n",
       "21           0.477778  \n",
       "22           1.000000  \n",
       "23           0.000000  \n",
       "24           0.000000  \n",
       "25           0.000000  \n",
       "26           1.000000  \n",
       "27           0.500000  \n",
       "28           0.000000  \n",
       "29           0.000000  \n",
       "..                ...  \n",
       "70           0.000000  \n",
       "71           0.000000  \n",
       "72           0.638889  \n",
       "73           0.000000  \n",
       "74           0.000000  \n",
       "75           1.000000  \n",
       "76           0.000000  \n",
       "77           0.755556  \n",
       "78           0.000000  \n",
       "79           0.500000  \n",
       "80           0.333333  \n",
       "81           1.000000  \n",
       "82           1.000000  \n",
       "83           0.000000  \n",
       "84           0.000000  \n",
       "85           0.000000  \n",
       "86           0.250000  \n",
       "87           1.000000  \n",
       "88           0.804167  \n",
       "89           0.000000  \n",
       "90           0.000000  \n",
       "91           0.500000  \n",
       "92           0.000000  \n",
       "93           0.500000  \n",
       "94           0.000000  \n",
       "95           0.000000  \n",
       "96           0.000000  \n",
       "97           0.000000  \n",
       "98           0.000000  \n",
       "99           0.000000  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting\n",
    "def tfidf_sentences_fit():\n",
    "    X = vectorizer.fit_transform(list(data.sentence))\n",
    "    return X\n",
    "\n",
    "# similarity\n",
    "def tfidf_sentences_predict(x, query):\n",
    "    pred = []\n",
    "    query = ''.join(Mystem().lemmatize(query.lower()))\n",
    "    vec = vectorizer.transform([query]).toarray()\n",
    "    simil = []\n",
    "    for vector, film, sent in zip(x, list(data.film), list(data.sentence)):\n",
    "        simil.append([1 - spatial.distance.cosine(vector.toarray(), vec), film, sent])\n",
    "    simil.sort(reverse=True)\n",
    "    for sim in simil[:5]:\n",
    "        pred.append(sim)\n",
    "    return pred\n",
    "\n",
    "df = retrieval(tfidf_sentences_fit, tfidf_sentences_predict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fs4PpI40l4q1",
    "outputId": "7d8cbe18-872a-49b2-a159-0775bb6bf47d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>embedding</th>\n",
       "      <th>recall</th>\n",
       "      <th>MAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.134667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stop-words, lemmatization</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.446333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>split_sentence, stop-words, lemmatization</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.138056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.457833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stop-words, lemmatization</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.565833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>split_sentence, stop-words, lemmatization</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.278486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               preprocessing        embedding  recall  \\\n",
       "0                                       None  CountVectorizer    0.22   \n",
       "1                  stop-words, lemmatization  CountVectorizer    0.58   \n",
       "2  split_sentence, stop-words, lemmatization  CountVectorizer    0.24   \n",
       "3                                       None           TF-IDF    0.67   \n",
       "4                  stop-words, lemmatization           TF-IDF    0.78   \n",
       "5  split_sentence, stop-words, lemmatization           TF-IDF    0.44   \n",
       "\n",
       "        MAP  \n",
       "0  0.134667  \n",
       "1  0.446333  \n",
       "2  0.138056  \n",
       "3  0.457833  \n",
       "4  0.565833  \n",
       "5  0.278486  "
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame({'preprocessing': 'split_sentence, stop-words, lemmatization',\n",
    "                    'embedding': 'TF-IDF',\n",
    "                    'recall': len([1 for prec in df.precision if prec != 0])/len(queries),\n",
    "                    'MAP': sum(df.average_precision)/len(queries)},\n",
    "                    index=[5])\n",
    "MAP = MAP.append(df2)\n",
    "MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4goZkk-l4q2"
   },
   "source": [
    "# Doc2Vec\n",
    "###  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pk2sqhHtl4q2",
    "outputId": "0dcbbb2d-6546-4fb8-f963-5ba783550aec"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c15d94961a30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtagged_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilm\u001b[0m  \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"\\w+\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstops\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtagged_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTaggedDocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtitles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "tagged_data = []\n",
    "for text, film  in zip(data.sentence, data.film):\n",
    "    try:\n",
    "        tagged_data.append(TaggedDocument(words=text, tags=[titles.index(film)]))\n",
    "    except AttributeError:\n",
    "        continue\n",
    "tagged_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVZxwHPLl4q3"
   },
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxiyTYbSl4q3",
    "outputId": "49e5e52e-f6f7-4565-d634-714a086723b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\merku\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "max_epochs = 20\n",
    "vec_size = 50\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "                min_count=5,\n",
    "                epochs=max_epochs)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, \n",
    "            total_examples=model.corpus_count, \n",
    "            epochs=model.epochs)\n",
    "model.save(\"d2v_sent.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQP7jWFgl4q3"
   },
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9nXr2jql4q4",
    "outputId": "70045208-35b0-49b2-ef4b-6b6aba45b18c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\merku\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec.load(\"d2v_sent.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdWlgz64l4q5",
    "outputId": "672410b0-fac4-4474-f736-98759d19e517",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-e65058715687>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc2vec_retrieval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-e65058715687>\u001b[0m in \u001b[0;36mdoc2vec_retrieval\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'query'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"\\w+\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstops\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0msimilar_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def doc2vec_retrieval():\n",
    "    df = pd.DataFrame(columns=['query', 'precision', 'average_precision'])\n",
    "    for index, query in enumerate(queries['query']):\n",
    "        test_data = [word for word in re.findall(r\"\\w+\", query.lower()) if word not in stops]\n",
    "        v1 = model.infer_vector(test_data)\n",
    "        similar_doc = model.docvecs.most_similar(positive=[v1], topn=5)\n",
    "        pred = []\n",
    "        for film, q in similar_doc:\n",
    "            pred.append([q, titles[int(film)]])\n",
    "        relev = [0] * 5\n",
    "        for i in range(5):\n",
    "            if pred[i][1] == queries.film[index]:\n",
    "                relev[i] = 1\n",
    "        if relev != [0] * 5:\n",
    "            df = df.append(evaluation(query, relev, index))\n",
    "        else:\n",
    "            df2 = pd.DataFrame({'query': query,\n",
    "                                'precision': 0.0,\n",
    "                                'average_precision': 0.0},\n",
    "                                index=[index])\n",
    "            df = df.append(df2)\n",
    "    return df\n",
    "\n",
    "df = doc2vec_retrieval()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1N0ovExgl4q5",
    "outputId": "0a4316a8-420e-4654-c663-67a631f223b6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>embedding</th>\n",
       "      <th>recall</th>\n",
       "      <th>MAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.134667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stop-words, lemmatization</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.446333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>split_sentence, stop-words, lemmatization</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.138056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.457833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stop-words, lemmatization</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.565833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>split_sentence, stop-words, lemmatization</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.278486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>split_sentence</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.198000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               preprocessing        embedding  recall  \\\n",
       "0                                       None  CountVectorizer    0.22   \n",
       "1                  stop-words, lemmatization  CountVectorizer    0.58   \n",
       "2  split_sentence, stop-words, lemmatization  CountVectorizer    0.24   \n",
       "3                                       None           TF-IDF    0.67   \n",
       "4                  stop-words, lemmatization           TF-IDF    0.78   \n",
       "5  split_sentence, stop-words, lemmatization           TF-IDF    0.44   \n",
       "6                             split_sentence          Doc2Vec    0.31   \n",
       "\n",
       "        MAP  \n",
       "0  0.134667  \n",
       "1  0.446333  \n",
       "2  0.138056  \n",
       "3  0.457833  \n",
       "4  0.565833  \n",
       "5  0.278486  \n",
       "6  0.198000  "
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame({'preprocessing': 'split_sentence',\n",
    "                    'embedding': 'Doc2Vec',\n",
    "                    'recall': len([1 for prec in df.precision if prec != 0])/len(queries),\n",
    "                    'MAP': sum(df.average_precision)/len(queries)},\n",
    "                    index=[6])\n",
    "MAP = MAP.append(df2)\n",
    "MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1P1btyiul4q6"
   },
   "source": [
    "#   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SX8E_AvMl4q6",
    "outputId": "ad060350-f9d3-4a9d-b2d8-deeba8e95406"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\merku\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>film</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>     ...</td>\n",
       "      <td> (Aladdin, 2019)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>     ...</td>\n",
       "      <td> (Aladdin, 2019)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>    ...</td>\n",
       "      <td> (Aladdin, 2019)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>     ...</td>\n",
       "      <td> (Aladdin, 2019)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>    </td>\n",
       "      <td> (Aladdin, 2019)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "index                                                      \n",
       "1           ...   \n",
       "1           ...   \n",
       "1          ...   \n",
       "1           ...   \n",
       "1                    \n",
       "\n",
       "                            film  \n",
       "index                             \n",
       "1       (Aladdin, 2019)  \n",
       "1       (Aladdin, 2019)  \n",
       "1       (Aladdin, 2019)  \n",
       "1       (Aladdin, 2019)  \n",
       "1       (Aladdin, 2019)  "
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame.from_csv('lemma_films.csv')\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaV2ImPul4q6"
   },
   "source": [
    "#   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1eBSwUyl4q7",
    "outputId": "5ed5370a-2152-4609-a25e-2769c1acd68a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\merku\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "titles = open('titles.txt', encoding='utf-8').read().split('\\n&&&\\n')\n",
    "\n",
    "tagged_data = []\n",
    "for text, film in zip(data.sentence, data.film):\n",
    "    try:\n",
    "        text = [word for word in re.findall(r\"\\w+\", text.lower()) if word not in stops]\n",
    "        tagged_data.append(TaggedDocument(words=text, tags=[titles.index(film)]))\n",
    "    except AttributeError:\n",
    "        continue\n",
    "\n",
    "# fitting\n",
    "max_epochs = 20\n",
    "vec_size = 50\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "                min_count=5,\n",
    "                epochs=max_epochs)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, \n",
    "            total_examples=model.corpus_count, \n",
    "            epochs=model.epochs)\n",
    "model.save(\"d2v_lemma.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQvaSC1-l4q7"
   },
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PbJZeZDUl4q7",
    "outputId": "95727896-97f0-4a23-f538-7e1396095f56",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>precision</th>\n",
       "      <th>average_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>       ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>     </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>     ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>         ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>       ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>          ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>    </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>   </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>        1...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>  </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>      </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>    </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>   </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>       </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>      </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>        ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>   </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>  </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>       ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>  90     ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>       ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>     </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>        ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>       ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>       ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>     </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>       ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>        ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>      </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>     ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>     ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>     ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>       ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>   </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>     </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>      </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>         ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>   </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>  it   </td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>    </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>     </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>      </td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>     ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>      ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  precision  \\\n",
       "0         ...        0.0   \n",
       "1         ...        0.0   \n",
       "2          ...        0.2   \n",
       "3                           0.0   \n",
       "4        ...        0.2   \n",
       "5         ...        0.0   \n",
       "6            ...        0.0   \n",
       "7         ...        0.2   \n",
       "8         ...        0.2   \n",
       "9          ...        0.0   \n",
       "10        ...        0.0   \n",
       "11        ...        0.2   \n",
       "12        ...        0.0   \n",
       "13            ...        0.0   \n",
       "14                                      0.0   \n",
       "15                                   0.2   \n",
       "16          1...        0.2   \n",
       "17                                        0.0   \n",
       "18                       0.2   \n",
       "19                                 0.2   \n",
       "20                                     0.0   \n",
       "21                   0.2   \n",
       "22        ...        0.2   \n",
       "23        ...        0.2   \n",
       "24        ...        0.0   \n",
       "25                          0.2   \n",
       "26          ...        0.2   \n",
       "27                               0.2   \n",
       "28                                    0.0   \n",
       "29         ...        0.0   \n",
       "..                                                ...        ...   \n",
       "70    90     ...        0.0   \n",
       "71         ...        0.0   \n",
       "72                                0.2   \n",
       "73          ...        0.0   \n",
       "74        ...        0.0   \n",
       "75         ...        0.0   \n",
       "76         ...        0.2   \n",
       "77                      0.2   \n",
       "78         ...        0.2   \n",
       "79          ...        0.2   \n",
       "80                       0.0   \n",
       "81       ...        0.2   \n",
       "82       ...        0.2   \n",
       "83        ...        0.0   \n",
       "84        ...        0.2   \n",
       "85       ...        0.0   \n",
       "86         ...        0.2   \n",
       "87                                        0.2   \n",
       "88                           0.2   \n",
       "89                    0.0   \n",
       "90           ...        0.0   \n",
       "91                                   0.0   \n",
       "92              it           0.0   \n",
       "93                     0.2   \n",
       "94        ...        0.0   \n",
       "95                                 0.2   \n",
       "96                   0.2   \n",
       "97       ...        0.0   \n",
       "98        ...        0.2   \n",
       "99        ...        0.0   \n",
       "\n",
       "    average_precision  \n",
       "0            0.000000  \n",
       "1            0.000000  \n",
       "2            1.000000  \n",
       "3            0.000000  \n",
       "4            0.333333  \n",
       "5            0.000000  \n",
       "6            0.000000  \n",
       "7            0.200000  \n",
       "8            0.333333  \n",
       "9            0.000000  \n",
       "10           0.000000  \n",
       "11           1.000000  \n",
       "12           0.000000  \n",
       "13           0.000000  \n",
       "14           0.000000  \n",
       "15           0.500000  \n",
       "16           1.000000  \n",
       "17           0.000000  \n",
       "18           1.000000  \n",
       "19           0.500000  \n",
       "20           0.000000  \n",
       "21           1.000000  \n",
       "22           1.000000  \n",
       "23           1.000000  \n",
       "24           0.000000  \n",
       "25           0.500000  \n",
       "26           1.000000  \n",
       "27           1.000000  \n",
       "28           0.000000  \n",
       "29           0.000000  \n",
       "..                ...  \n",
       "70           0.000000  \n",
       "71           0.000000  \n",
       "72           1.000000  \n",
       "73           0.000000  \n",
       "74           0.000000  \n",
       "75           0.000000  \n",
       "76           0.333333  \n",
       "77           1.000000  \n",
       "78           1.000000  \n",
       "79           1.000000  \n",
       "80           0.000000  \n",
       "81           1.000000  \n",
       "82           0.200000  \n",
       "83           0.000000  \n",
       "84           0.500000  \n",
       "85           0.000000  \n",
       "86           1.000000  \n",
       "87           0.200000  \n",
       "88           1.000000  \n",
       "89           0.000000  \n",
       "90           0.000000  \n",
       "91           0.000000  \n",
       "92           0.000000  \n",
       "93           1.000000  \n",
       "94           0.000000  \n",
       "95           0.500000  \n",
       "96           0.333333  \n",
       "97           0.000000  \n",
       "98           0.333333  \n",
       "99           0.000000  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def doc2vec_improved_retrieval():\n",
    "    df = pd.DataFrame(columns=['query', 'precision', 'average_precision'])\n",
    "    for index, query in enumerate(queries['query']):\n",
    "        query = ' '.join(re.findall(r\"\\w+\", query.lower()))\n",
    "        test_data = [word for word in Mystem().lemmatize(query) if word not in stops]\n",
    "        v1 = model.infer_vector(test_data)\n",
    "        similar_doc = model.docvecs.most_similar(positive=[v1], topn=5)\n",
    "        pred = []\n",
    "        for film, q in similar_doc:\n",
    "            pred.append([q, titles[int(film)]])\n",
    "        relev = [0] * 5\n",
    "        for i in range(5):\n",
    "            if pred[i][1] == queries.film[index]:\n",
    "                relev[i] = 1\n",
    "        if relev != [0] * 5:\n",
    "            df = df.append(evaluation(query, relev, index))\n",
    "        else:\n",
    "            df2 = pd.DataFrame({'query': query,\n",
    "                                'precision': 0.0,\n",
    "                                'average_precision': 0.0},\n",
    "                                index=[index])\n",
    "            df = df.append(df2)\n",
    "    return df\n",
    "\n",
    "model = Doc2Vec.load(\"d2v_lemma.model\")\n",
    "df = doc2vec_improved_retrieval()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eRWC-7Nel4q8",
    "outputId": "6bc59026-acc2-4d22-eeb4-29fdf9d46848"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>embedding</th>\n",
       "      <th>recall</th>\n",
       "      <th>MAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.134667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stop-words, lemmatization</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.446333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>split_sentence, stop-words, lemmatization</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.138056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.457833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stop-words, lemmatization</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.565833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>split_sentence, stop-words, lemmatization</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.278486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>split_sentence</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.198000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>split_sentence, stop-words, lemmatization</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.380000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               preprocessing        embedding  recall  \\\n",
       "0                                       None  CountVectorizer    0.22   \n",
       "1                  stop-words, lemmatization  CountVectorizer    0.58   \n",
       "2  split_sentence, stop-words, lemmatization  CountVectorizer    0.24   \n",
       "3                                       None           TF-IDF    0.67   \n",
       "4                  stop-words, lemmatization           TF-IDF    0.78   \n",
       "5  split_sentence, stop-words, lemmatization           TF-IDF    0.44   \n",
       "6                             split_sentence          Doc2Vec    0.31   \n",
       "7  split_sentence, stop-words, lemmatization          Doc2Vec    0.56   \n",
       "\n",
       "        MAP  \n",
       "0  0.134667  \n",
       "1  0.446333  \n",
       "2  0.138056  \n",
       "3  0.457833  \n",
       "4  0.565833  \n",
       "5  0.278486  \n",
       "6  0.198000  \n",
       "7  0.380000  "
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame({'preprocessing': 'split_sentence, stop-words, lemmatization',\n",
    "                    'embedding': 'Doc2Vec',\n",
    "                    'recall': len([1 for prec in df.precision if prec != 0])/len(queries),\n",
    "                    'MAP': sum(df.average_precision)/len(queries)},\n",
    "                    index=[7])\n",
    "MAP = MAP.append(df2)\n",
    "MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqkIDdwIl4q8"
   },
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AWFttSXl4q9",
    "outputId": "be649179-16d1-4456-a109-2109073fee47",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_improved_fit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d8a490263939>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munit_retrieval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-d8a490263939>\u001b[0m in \u001b[0;36munit_retrieval\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'query'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'precision'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'average_precision'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'query'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_improved_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_improved_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"\\w+\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_improved_fit' is not defined"
     ]
    }
   ],
   "source": [
    "def unit_retrieval():\n",
    "    df = pd.DataFrame(columns=['query', 'precision', 'average_precision'])\n",
    "    for index, query in enumerate(queries['query']):\n",
    "        x = tfidf_improved_fit()\n",
    "        pred = tfidf_improved_predict(x, query)\n",
    "        query = ' '.join(re.findall(r\"\\w+\", query.lower()))\n",
    "        test_data = [word for word in Mystem().lemmatize(query) if word not in stops]\n",
    "        v1 = model.infer_vector(test_data)\n",
    "        similar_doc = model.docvecs.most_similar(positive=[v1], topn=5)\n",
    "        for film, q in similar_doc:\n",
    "            pred.append([q, titles[int(film)]])\n",
    "        sum_score = sum([q[0] for q in pred]) \n",
    "        for i, film in enumerate(pred):\n",
    "            pred[i] = [pred[i][0]/sum_score, pred[i][1]]\n",
    "        pred.sort(reverse=True)\n",
    "        relev = [0] * 5\n",
    "        for i, pr in enumerate(pred[:5]):\n",
    "            if pr[1] == queries.film[index]:\n",
    "                relev[i] = 1\n",
    "        if relev != [0] * 5:   \n",
    "            df = df.append(evaluation(query, relev, index, k=5))\n",
    "        else:\n",
    "            df2 = pd.DataFrame({'query': query,\n",
    "                                'precision': 0.0,\n",
    "                                'average_precision': 0.0},\n",
    "                                index=[index])\n",
    "            df = df.append(df2)\n",
    "    return df\n",
    "\n",
    "df = unit_retrieval()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kivKQn4yl4q9",
    "outputId": "d8b20bd6-666c-4211-d0cc-f1f16fc69e0b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>embedding</th>\n",
       "      <th>recall</th>\n",
       "      <th>MAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.134667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stop-words, lemmatization</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.446333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>split_sentence, stop-words, lemmatization</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.138056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.457833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stop-words, lemmatization</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.565833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>split_sentence, stop-words, lemmatization</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.278486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>split_sentence</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.198000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>split_sentence, stop-words, lemmatization</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>split_sentence, stop-words, lemmatization</td>\n",
       "      <td>Doc2Vec &amp; TF-IDF</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.413000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               preprocessing         embedding  recall  \\\n",
       "0                                       None   CountVectorizer    0.22   \n",
       "1                  stop-words, lemmatization   CountVectorizer    0.58   \n",
       "2  split_sentence, stop-words, lemmatization   CountVectorizer    0.24   \n",
       "3                                       None            TF-IDF    0.67   \n",
       "4                  stop-words, lemmatization            TF-IDF    0.78   \n",
       "5  split_sentence, stop-words, lemmatization            TF-IDF    0.44   \n",
       "6                             split_sentence           Doc2Vec    0.31   \n",
       "7  split_sentence, stop-words, lemmatization           Doc2Vec    0.56   \n",
       "8  split_sentence, stop-words, lemmatization  Doc2Vec & TF-IDF    0.56   \n",
       "\n",
       "        MAP  \n",
       "0  0.134667  \n",
       "1  0.446333  \n",
       "2  0.138056  \n",
       "3  0.457833  \n",
       "4  0.565833  \n",
       "5  0.278486  \n",
       "6  0.198000  \n",
       "7  0.380000  \n",
       "8  0.413000  "
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame({'preprocessing': 'split_sentence, stop-words, lemmatization',\n",
    "                    'embedding': 'Doc2Vec & TF-IDF',\n",
    "                    'recall': len([1 for prec in df.precision if prec != 0])/len(queries),\n",
    "                    'MAP': sum(df.average_precision)/len(queries)},\n",
    "                    index=[8])\n",
    "MAP = MAP.append(df2)\n",
    "MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwDaZAK_l4q-",
    "outputId": "776c1d23-af25-4cb3-9619-66eb70cc7427"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>precision</th>\n",
       "      <th>average_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>    </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>   </td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             query  precision  average_precision\n",
       "19              0.2                1.0\n",
       "20                  0.2                1.0"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[19:21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwAXPzqul4q-"
   },
   "source": [
    "# One query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJRjz1h8l4q-"
   },
   "source": [
    "query = input()\n",
    "x = tfidf_improved_fit()\n",
    "pred = tfidf_improved_predict(x, query)\n",
    "query = ' '.join(re.findall(r\"\\w+\", query.lower()))\n",
    "test_data = [word for word in Mystem().lemmatize(query) if word not in stops]\n",
    "v1 = model.infer_vector(test_data)\n",
    "similar_doc = model.docvecs.most_similar(positive=[v1], topn=5)\n",
    "for film, q in similar_doc:\n",
    "    pred.append([q, titles[int(film)]])\n",
    "    pred.sort(reverse=True)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qh7ZD9nEl4q-"
   },
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2MstGC7l4q_",
    "outputId": "4a8aa5a0-3070-4e6e-a6e1-4193ddf831be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7679873108863831"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1 = re.findall(r'\\w+', \"\")\n",
    "test_2 = re.findall(r'\\w+', \"\")\n",
    "v1 = model.infer_vector(test_1)\n",
    "v2 = model.infer_vector(test_2)\n",
    "cos_sim = 1 - spatial.distance.cosine(v1, v2)\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fM3mtqYl4q_",
    "outputId": "356c1e40-f70b-46f7-e68b-6dd42d32bdd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.784702718257904"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1 = re.findall(r'\\w+', \"\")\n",
    "test_2 = re.findall(r'\\w+', \"\")\n",
    "v1 = model.infer_vector(test_1)\n",
    "v2 = model.infer_vector(test_2)\n",
    "cos_sim = 1 - spatial.distance.cosine(v1, v2)\n",
    "cos_sim"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "doc2vec_processor.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
