{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install pandas pytorch_lightning pytorch_metric_learning wandb fastparquet\n",
    "!pip install sklearn torch transformers sentence-transformers\n",
    "# !pip3 install torch==1.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Domain adoptation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer, models, datasets, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Define your sentence transformer model using CLS pooling\n",
    "model_name = 'paraphrase-MiniLM-L12-v2'\n",
    "word_embedding_model = models.Transformer(model_name, max_seq_length=64)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), 'cls')\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "# Define a list with sentences (1k - 100k sentences)\n",
    "data_path = Path(r'..\\1. crawling & parsing\\wiki_film_descriptions\\film_plots.txt')\n",
    "train_sentences = []\n",
    "texts = open(data_path, encoding='utf-8').read()\n",
    "for text in tqdm(texts.split('\\n')):\n",
    "    train_sentences.extend(nltk.sent_tokenize(text, 'russian'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TSDAE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the special denoising dataset that adds noise on-the-fly\n",
    "train_dataset = datasets.DenoisingAutoEncoderDataset(train_sentences)\n",
    "\n",
    "# DataLoader to batch your data\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Use the denoising auto-encoder loss\n",
    "train_loss = losses.DenoisingAutoEncoderLoss(model, decoder_name_or_path=model_name, tie_encoder_decoder=True)\n",
    "\n",
    "# Call the fit method\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=1,\n",
    "    weight_decay=0,\n",
    "    scheduler='constantlr',\n",
    "    optimizer_params={'lr': 3e-5},\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "model.save('output/tsdae-model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Similarity VK - Wiki"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, InputExample, evaluation, models, datasets, losses\n",
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_path = Path(r'..\\1. crawling & parsing\\wiki_film_descriptions')\n",
    "\n",
    "wiki_data = open(train_path / 'film_plots.txt', encoding='utf-8').read().split('\\n')\n",
    "wiki_labels = open(train_path / 'wiki_titles.txt', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "test_path = Path(r'..\\1. crawling & parsing\\vk_test_queries')\n",
    "\n",
    "vk_data = open(test_path / 'test_data.txt', encoding='utf-8').read().split('\\n')\n",
    "vk_labels = open(test_path / 'test_titles.txt', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "_, _, vk_test_titles, _ = open(r'data_test_100\\data_100.txt', encoding='utf-8').read().split('\\n&&&\\n')\n",
    "vk_test_titles = vk_test_titles.split('\\n')\n",
    "\n",
    "negative_pairs = pd.read_csv('negative_pairs.tsv', sep='\\t')\n",
    "train_pairs = []\n",
    "val_pairs = []\n",
    "\n",
    "for query_labels, query_text in zip(vk_labels, vk_data):\n",
    "    for query_label in query_labels.split(';'):\n",
    "        wiki_label_id = wiki_labels.index(query_label)\n",
    "        if query_labels not in vk_test_titles:\n",
    "            train_pairs.append((query_text, wiki_data[wiki_label_id], 1))\n",
    "        else:\n",
    "            val_pairs.append((query_text, wiki_data[wiki_label_id], 1))\n",
    "\n",
    "# todo split to train and 100 val queries\n",
    "for _, row in negative_pairs.iterrows():\n",
    "    vk_label, wiki_label = row.values\n",
    "    wiki_label_id = wiki_labels.index(wiki_label)\n",
    "    vk_label_ids = [i for i, label in enumerate(vk_labels) if vk_label in label]\n",
    "    for vk_label_id in vk_label_ids:\n",
    "        if vk_label_id < 100:\n",
    "            val_pairs.append((vk_data[vk_label_id], wiki_data[wiki_label_id], 0))\n",
    "        else:\n",
    "            train_pairs.append((vk_data[vk_label_id], wiki_data[wiki_label_id], 0))\n",
    "\n",
    "train_df = pd.DataFrame(train_pairs, columns=['vk', 'wiki', 'label']).drop_duplicates().reset_index().drop(columns=['index'])\n",
    "val_df = pd.DataFrame(train_pairs, columns=['vk', 'wiki', 'label']).drop_duplicates().reset_index().drop(columns=['index'])\n",
    "train_df.to_csv('mined_pairs_test_100.tsv', sep='\\t', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the model. Either from scratch of by loading a pre-trained model\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "# Define your train examples.\n",
    "train_examples = [InputExample(texts=row.values[:2], label=row.values[2])\n",
    "                  for _, row in train_df.iterrows()]\n",
    "# Define your train dataset, the dataloader and the train loss\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.ContrastiveLoss(model=model)\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator(\n",
    "    val_df['vk'].to_list(), val_df['wiki'].to_list(), val_df['label'].to_list())\n",
    "# Tune the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=20, warmup_steps=100,\n",
    "          show_progress_bar=True, evaluator=evaluator, evaluation_steps=1000, output_path='adopted_model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yandex DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "519d2598-b631-410c-9149-ba1bca02b44d",
  "notebookPath": "product_categorization/dev/MetricText.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}